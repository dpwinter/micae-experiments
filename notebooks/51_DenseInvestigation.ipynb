{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fully Convolutional autoencoder (Enc+Dec, 1-Split, 16-dim) on Reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "pt_conv1 (Conv2D)            (None, 4, 4, 1)           257       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "latent (Activation)          (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 19,585\n",
      "Trainable params: 19,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dec_input (InputLayer)       [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 5, 5, 1)           5         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 10, 10, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 12, 12, 64)        640       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 1)         33        \n",
      "=================================================================\n",
      "Total params: 51,910\n",
      "Trainable params: 51,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, MaxPooling2D, UpSampling2D\n",
    "from keras.utils import plot_model\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "# encoder\n",
    "enc_input = Input(shape=(28,28,1), name='enc_input')\n",
    "x = Conv2D(filters=32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='valid', name='conv1')(enc_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='valid', name='conv2')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(filters=1, kernel_size=(2,2), strides=(1,1), activation='relu', padding='valid', name='pt_conv1')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "# latent = Activation('sigmoid', name='latent')(x)\n",
    "latent = Activation('linear', name='latent')(x)\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Reshape(target_shape=(4,4,1), name='reshape')(dec_input)\n",
    "x = Conv2DTranspose(filters=1, kernel_size=(2,2), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = UpSampling2D()(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = UpSampling2D()(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "dec_output = Conv2D(filters=1, kernel_size=1, padding='same')(x)\n",
    "\n",
    "encoder = Model(enc_input, latent, name=\"Encoder\")\n",
    "decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "model = Model(encoder.input, decoder(encoder.output))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = keras.models.load_model('../models/51_1split_2.h5')\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "# mc = ModelCheckpoint('../models/51_1split_2.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "# history = model.fit(x_train, x_train, validation_data=(x_test, x_test), epochs=200, batch_size=32, callbacks=[mc])\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('No-Split training')\n",
    "# plt.ylabel('MSE loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper right')\n",
    "# plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "# plt.savefig('../img/51_PLOTS/FullyConv1split_loss_3.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f4c7869ff10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 22 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "n = 7\n",
    "plt.figure(figsize=(15, 5))\n",
    "axs = []\n",
    "for i in range(n):\n",
    "    \n",
    "    # predict\n",
    "    x = x_test[i]\n",
    "    z = encoder(x.reshape(1,28,28,1))\n",
    "    y = decoder(z)\n",
    "    \n",
    "    axs.append(plt.subplot(3, n, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape(28,28), vmin=0, vmax=1)\n",
    "    \n",
    "    axs.append(plt.subplot(3, n, n+i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(tf.reshape(z, (4,4)), vmin=0, vmax=1)\n",
    "    \n",
    "    axs.append(plt.subplot(3, n, 2*n+i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(tf.reshape(y, (28,28)), vmin=0, vmax=1)\n",
    "plt.colorbar(ax=axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Architecture from above with Dense added at end of encoder and start of decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "pt_conv1 (Conv2D)            (None, 4, 4, 1)           257       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "=================================================================\n",
      "Total params: 19,857\n",
      "Trainable params: 19,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dec_input (InputLayer)       [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 5, 5, 1)           5         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 10, 10, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 12, 12, 64)        640       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 1)         33        \n",
      "=================================================================\n",
      "Total params: 52,182\n",
      "Trainable params: 52,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, MaxPooling2D, UpSampling2D\n",
    "from keras.utils import plot_model\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "# encoder\n",
    "enc_input = Input(shape=(28,28,1), name='enc_input')\n",
    "x = Conv2D(filters=32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='valid', name='conv1')(enc_input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='valid', name='conv2')(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(filters=1, kernel_size=(2,2), strides=(1,1), activation='relu', padding='valid', name='pt_conv1')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "# latent = Dense(activation='sigmoid', units=16)(x)\n",
    "latent = Dense(activation='linear', units=16)(x)\n",
    "# latent = Activation('sigmoid', name='latent')(x)\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Dense(units=16)(dec_input)\n",
    "x = Reshape(target_shape=(4,4,1), name='reshape')(x)\n",
    "x = Conv2DTranspose(filters=1, kernel_size=(2,2), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = UpSampling2D()(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = UpSampling2D()(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=(5,5), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "dec_output = Conv2D(filters=1, kernel_size=1, padding='same')(x)\n",
    "\n",
    "encoder = Model(enc_input, latent, name=\"Encoder\")\n",
    "decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "model = Model(encoder.input, decoder(encoder.output))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(encoder.summary())\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00001: val_loss improved from inf to 0.01688, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 2/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00002: val_loss improved from 0.01688 to 0.01682, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 3/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00003: val_loss did not improve from 0.01682\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 4/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00004: val_loss did not improve from 0.01682\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 5/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00005: val_loss did not improve from 0.01682\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 6/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00006: val_loss did not improve from 0.01682\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 7/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00007: val_loss improved from 0.01682 to 0.01674, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 8/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00008: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 9/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00009: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 10/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00010: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 11/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00011: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 12/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00012: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 13/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00013: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 14/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00014: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 15/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00015: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 16/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00016: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 17/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00017: val_loss did not improve from 0.01674\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 18/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00018: val_loss improved from 0.01674 to 0.01672, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 19/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00019: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 20/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00020: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 21/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00021: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 22/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00022: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 23/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00023: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 24/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00024: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 25/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00025: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 26/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00026: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 27/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00027: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 28/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00028: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 29/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00029: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 30/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00030: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 31/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00031: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0179\n",
      "Epoch 32/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00032: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 33/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00033: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 34/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00034: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 35/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00035: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 36/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00036: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 37/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00037: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 38/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00038: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 39/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00039: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 40/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00040: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 41/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00041: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 42/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00042: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 43/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00043: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 44/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00044: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 45/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00045: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 46/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00046: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 47/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00047: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 48/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00048: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 49/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00049: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 50/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00050: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0172\n",
      "Epoch 51/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00051: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 52/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00052: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 53/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00053: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 54/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00054: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 55/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00055: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 56/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00056: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0171\n",
      "Epoch 57/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00057: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 58/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00058: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 59/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00059: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 60/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00060: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 61/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00061: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 62/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00062: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 63/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00063: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 64/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00064: val_loss did not improve from 0.01672\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 65/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00065: val_loss improved from 0.01672 to 0.01667, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 66/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00066: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 67/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00067: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 68/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00068: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 69/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00069: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 70/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00070: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 71/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00071: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 72/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00072: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 73/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00073: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 74/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00074: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 75/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00075: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 76/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00076: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 77/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00077: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 78/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00078: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 79/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00079: val_loss did not improve from 0.01667\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 80/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00080: val_loss improved from 0.01667 to 0.01665, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 81/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00081: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0173\n",
      "Epoch 82/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00082: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 83/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00083: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 84/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00084: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 85/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00085: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 86/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00086: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 87/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00087: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 88/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00088: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 89/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00089: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 90/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00090: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 91/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00091: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 92/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00092: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 93/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00093: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 94/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00094: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 95/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00095: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 96/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00096: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 97/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00097: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 98/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00098: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 99/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00099: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 100/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00100: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 101/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00101: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0172\n",
      "Epoch 102/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00102: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 103/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00103: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 104/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00104: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 105/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00105: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 106/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00106: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 107/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00107: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 108/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00108: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 109/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00109: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0175\n",
      "Epoch 110/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00110: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 111/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00111: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0172\n",
      "Epoch 112/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00112: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 113/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00113: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 114/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00114: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 115/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00115: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 116/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00116: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 117/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00117: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 118/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00118: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 119/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00119: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 120/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00120: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 121/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00121: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 122/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00122: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 123/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00123: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 124/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00124: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 125/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00125: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 126/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00126: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 127/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00127: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 128/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00128: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0172\n",
      "Epoch 129/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00129: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 130/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00130: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 131/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00131: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 132/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00132: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 133/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00133: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 134/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00134: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 135/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00135: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 136/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00136: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 137/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00137: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 138/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00138: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 139/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00139: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 140/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00140: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 141/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00141: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 142/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00142: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 143/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00143: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 144/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00144: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 145/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00145: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 146/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00146: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 147/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00147: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 148/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00148: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 149/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00149: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 150/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00150: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 151/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00151: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 152/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00152: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 153/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00153: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 154/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00154: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 155/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00155: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 156/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00156: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 157/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00157: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 158/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00158: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 159/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00159: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 160/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00160: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 161/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00161: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 162/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00162: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 163/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00163: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 164/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00164: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 165/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00165: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 166/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00166: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 167/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00167: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 168/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00168: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 169/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00169: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 170/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00170: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 171/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00171: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 172/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00172: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 173/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00173: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 174/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00174: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 175/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00175: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 176/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00176: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 177/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00177: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 178/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00178: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 179/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00179: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 180/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00180: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 181/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00181: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 182/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00182: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 183/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00183: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 184/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00184: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 185/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00185: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 186/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00186: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 187/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00187: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 188/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00188: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 189/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00189: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 190/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00190: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 191/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00191: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 192/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00192: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 193/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00193: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 194/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00194: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 195/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00195: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0173\n",
      "Epoch 196/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00196: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 197/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00197: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 198/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00198: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 199/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00199: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 200/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00200: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 201/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00201: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 202/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00202: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 203/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00203: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 204/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00204: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 205/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00205: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 206/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00206: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 207/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00207: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 208/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00208: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 209/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00209: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 210/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00210: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 211/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00211: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 212/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00212: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 213/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00213: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0171\n",
      "Epoch 214/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00214: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 215/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00215: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 216/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00216: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 217/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00217: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 218/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00218: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 219/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00219: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 220/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00220: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 221/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00221: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 222/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00222: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 223/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00223: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 224/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00224: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 225/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00225: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 226/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00226: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 227/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00227: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 228/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00228: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 229/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00229: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 230/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00230: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 231/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00231: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 232/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00232: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0173\n",
      "Epoch 233/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00233: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 234/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00234: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 235/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00235: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 236/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00236: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 237/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00237: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 238/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00238: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 239/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00239: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 240/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00240: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 241/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00241: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 242/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00242: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 243/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00243: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 244/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00244: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 245/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00245: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 246/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00246: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 247/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00247: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 248/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00248: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 249/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00249: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 250/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00250: val_loss did not improve from 0.01665\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 251/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00251: val_loss improved from 0.01665 to 0.01662, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 252/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00252: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 253/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00253: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 254/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00254: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 255/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00255: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 256/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00256: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 257/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00257: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 258/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00258: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 259/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00259: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 260/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00260: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 261/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00261: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 262/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00262: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 263/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00263: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 264/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00264: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 265/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00265: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 266/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00266: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 267/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00267: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 268/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00268: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 269/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00269: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 270/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00270: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 271/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00271: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 272/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00272: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 273/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00273: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 274/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00274: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 275/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00275: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 276/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00276: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 277/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00277: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 278/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00278: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 279/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00279: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 280/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00280: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 281/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00281: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 282/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00282: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 283/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00283: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 284/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00284: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 285/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00285: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 286/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00286: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 287/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00287: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 288/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00288: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 289/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00289: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 290/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00290: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 291/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00291: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 292/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00292: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 293/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00293: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 294/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00294: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 295/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00295: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 296/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00296: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 297/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00297: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 298/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00298: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 299/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00299: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 300/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00300: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 301/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00301: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 302/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00302: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 303/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00303: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 304/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00304: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 305/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00305: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 306/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00306: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 307/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00307: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 308/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00308: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 309/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00309: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 310/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00310: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 311/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00311: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 312/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00312: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 313/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00313: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 314/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00314: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 315/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00315: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 316/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00316: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 317/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00317: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 318/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00318: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 319/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00319: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 320/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00320: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 321/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00321: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 322/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00322: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 323/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00323: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 324/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00324: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 325/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00325: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 326/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00326: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 327/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00327: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 328/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00328: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 329/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00329: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 330/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00330: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 331/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00331: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 332/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00332: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 333/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00333: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 334/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00334: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 335/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00335: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 336/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00336: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 337/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00337: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 338/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00338: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 339/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00339: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 340/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00340: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 341/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00341: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 342/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00342: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 343/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00343: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 344/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00344: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 345/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00345: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 346/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00346: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 347/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00347: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 348/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00348: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 349/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00349: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 350/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00350: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 351/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00351: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 352/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00352: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 353/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00353: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 354/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00354: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 355/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00355: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 356/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00356: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 357/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00357: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 358/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00358: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 359/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00359: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 360/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00360: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 361/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00361: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 362/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00362: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 363/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00363: val_loss did not improve from 0.01662\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 364/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00364: val_loss improved from 0.01662 to 0.01660, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 365/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00365: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 366/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00366: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 367/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00367: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 368/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00368: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 369/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00369: val_loss improved from 0.01660 to 0.01656, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 370/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00370: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 371/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00371: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 372/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00372: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 373/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00373: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 374/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00374: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 375/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00375: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 376/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00376: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 377/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00377: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 378/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00378: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 379/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00379: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 380/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00380: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 381/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00381: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 382/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00382: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 383/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00383: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 384/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00384: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 385/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00385: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 386/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00386: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 387/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00387: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 388/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00388: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 389/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00389: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 390/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00390: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 391/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00391: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 392/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00392: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 393/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00393: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 394/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00394: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 395/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00395: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 396/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00396: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 397/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00397: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 398/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00398: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 399/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00399: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 400/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00400: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 401/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00401: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 402/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00402: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 403/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00403: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 404/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00404: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 405/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00405: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 406/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00406: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 407/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00407: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 408/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00408: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 409/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00409: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 410/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00410: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 411/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00411: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 412/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00412: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0170\n",
      "Epoch 413/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00413: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 414/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00414: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 415/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00415: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 416/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00416: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 417/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00417: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 418/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00418: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 419/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00419: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 420/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00420: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 421/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00421: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 422/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00422: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 423/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00423: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 424/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00424: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 425/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00425: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 426/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00426: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 427/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00427: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 428/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00428: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 429/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00429: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 430/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00430: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 431/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00431: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 432/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00432: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 433/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00433: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 434/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00434: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 435/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00435: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 436/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00436: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 437/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00437: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 438/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00438: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 439/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00439: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 440/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00440: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 441/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00441: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 442/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00442: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 443/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00443: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 444/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00444: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 445/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00445: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 446/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00446: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 447/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167- ETA: 0s\n",
      "Epoch 00447: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 448/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00448: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 449/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00449: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 450/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00450: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 451/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00451: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 452/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00452: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 453/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00453: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 454/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00454: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 455/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00455: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 456/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00456: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 457/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00457: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0169\n",
      "Epoch 458/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00458: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 459/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00459: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 460/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00460: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 461/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00461: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 462/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00462: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 463/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00463: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 464/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00464: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 465/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00465: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 466/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00466: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 467/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00467: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 468/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00468: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 469/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00469: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 470/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00470: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 471/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00471: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 472/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00472: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 473/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00473: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 474/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00474: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 475/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00475: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 476/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00476: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 477/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00477: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 478/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00478: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 479/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00479: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 480/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00480: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 481/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00481: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 482/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00482: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 483/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00483: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 484/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00484: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 485/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00485: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 486/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00486: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 487/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00487: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 488/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00488: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 489/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00489: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 490/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00490: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 491/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00491: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 492/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00492: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 493/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00493: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 494/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00494: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0174\n",
      "Epoch 495/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00495: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 496/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00496: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 497/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00497: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 498/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00498: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 499/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.016 - ETA: 0s - loss: 0.0167\n",
      "Epoch 00499: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 500/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167- ETA: 0s - loss: \n",
      "Epoch 00500: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 501/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00501: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 502/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00502: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 503/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00503: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 504/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00504: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 505/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00505: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 506/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00506: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 507/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00507: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 508/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00508: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0172\n",
      "Epoch 509/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00509: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 510/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00510: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 511/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00511: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 512/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00512: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 513/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00513: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 514/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00514: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 515/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00515: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 516/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00516: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 517/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00517: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 518/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00518: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 519/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00519: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 520/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00520: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 521/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00521: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 522/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00522: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 523/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00523: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 524/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00524: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 525/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00525: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 526/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00526: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 527/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00527: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 528/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00528: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 529/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00529: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 530/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00530: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 531/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00531: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 532/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00532: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 533/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00533: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 534/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00534: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 535/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00535: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 536/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167- ETA: \n",
      "Epoch 00536: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 537/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00537: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 538/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00538: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 539/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00539: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 540/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00540: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 541/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00541: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 542/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00542: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 543/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00543: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 544/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00544: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0168 - val_loss: 0.0172\n",
      "Epoch 545/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00545: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 546/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167- ETA: 0s \n",
      "Epoch 00546: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 547/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00547: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 548/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00548: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 549/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00549: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 550/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00550: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 551/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00551: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 552/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00552: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 553/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00553: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 554/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00554: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 555/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00555: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 556/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00556: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 557/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00557: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 558/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00558: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 559/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00559: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 560/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00560: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 561/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00561: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 562/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00562: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 563/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00563: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 564/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00564: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 565/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00565: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 566/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00566: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 567/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00567: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 568/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00568: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 569/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00569: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 570/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00570: val_loss improved from 0.01656 to 0.01656, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 571/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00571: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 572/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00572: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 573/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00573: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 574/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00574: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 575/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00575: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 576/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00576: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 577/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00577: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 578/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00578: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 579/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00579: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 580/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00580: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 581/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00581: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 582/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00582: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 583/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00583: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 584/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00584: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 585/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00585: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 586/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00586: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 587/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00587: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 588/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00588: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 589/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00589: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 590/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00590: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 591/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00591: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 592/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00592: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 593/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00593: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 594/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00594: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 595/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00595: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 596/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00596: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 597/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00597: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 598/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00598: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 599/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00599: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 600/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00600: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 601/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00601: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 602/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00602: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 603/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00603: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 604/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00604: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 605/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00605: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 606/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00606: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 607/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00607: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 608/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00608: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 609/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00609: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 610/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00610: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 611/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00611: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 612/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00612: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 613/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00613: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 614/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00614: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 615/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00615: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 616/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00616: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 617/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00617: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 618/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00618: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 619/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00619: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 620/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00620: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 621/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00621: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 622/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00622: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 623/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00623: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 624/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00624: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 625/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00625: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 626/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00626: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 627/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00627: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 628/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00628: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 629/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00629: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 630/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00630: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 631/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00631: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 632/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00632: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 633/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00633: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 634/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00634: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 635/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00635: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 636/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00636: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 637/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00637: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 638/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00638: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 639/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00639: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 640/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00640: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 641/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00641: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 642/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00642: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 643/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00643: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 644/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00644: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 645/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00645: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 646/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00646: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 647/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00647: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 648/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00648: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 649/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00649: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 650/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00650: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0171\n",
      "Epoch 651/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00651: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 652/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00652: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 653/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00653: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 654/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00654: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 655/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00655: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 656/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00656: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 657/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00657: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 658/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00658: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 659/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00659: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 660/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00660: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 661/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00661: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 662/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00662: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 663/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00663: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 664/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00664: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 665/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00665: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 666/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00666: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 667/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00667: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 668/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00668: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 669/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00669: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 670/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00670: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 671/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00671: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 672/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00672: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 673/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00673: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 674/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00674: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 675/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00675: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 676/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00676: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 677/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00677: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 678/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00678: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 679/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00679: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 680/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00680: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 681/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00681: val_loss did not improve from 0.01656\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 682/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00682: val_loss improved from 0.01656 to 0.01652, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0165\n",
      "Epoch 683/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00683: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 684/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00684: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 685/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00685: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 686/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00686: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 687/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00687: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 688/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00688: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 689/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00689: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 690/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00690: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 691/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00691: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 692/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00692: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 693/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00693: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 694/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00694: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 695/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00695: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 696/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00696: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 697/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00697: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 698/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00698: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 699/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00699: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 700/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00700: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 701/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00701: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 702/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00702: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 703/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00703: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 704/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00704: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 705/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00705: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 706/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00706: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 707/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00707: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 708/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00708: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 709/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00709: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 710/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00710: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 711/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00711: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 712/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00712: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 713/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00713: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 714/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00714: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 715/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00715: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 716/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00716: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0169\n",
      "Epoch 717/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00717: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 718/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00718: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 719/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00719: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 720/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00720: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 721/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00721: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 722/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00722: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 723/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00723: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 724/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00724: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 725/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00725: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 726/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166- ETA: 0s - loss: 0.016 - ETA: 0s - los\n",
      "Epoch 00726: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 727/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00727: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 728/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00728: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 729/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00729: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 730/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00730: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 731/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00731: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0165\n",
      "Epoch 732/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00732: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 733/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00733: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0174\n",
      "Epoch 734/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00734: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 735/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00735: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 736/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00736: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 737/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00737: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 738/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00738: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 739/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00739: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 740/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00740: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 741/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00741: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 742/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00742: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 743/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00743: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 744/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00744: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 745/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00745: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 746/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00746: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 747/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00747: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 748/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00748: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 749/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00749: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 750/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00750: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 751/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00751: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 752/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00752: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 753/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00753: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 754/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00754: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 755/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00755: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 756/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00756: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 757/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00757: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0168\n",
      "Epoch 758/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00758: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 759/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00759: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 760/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00760: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 761/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00761: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 762/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00762: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 763/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00763: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 764/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00764: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 765/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00765: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 766/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00766: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 767/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00767: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 768/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00768: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 769/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00769: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 770/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00770: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 771/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00771: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 772/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00772: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 773/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00773: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 774/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00774: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 775/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00775: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 776/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00776: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 777/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00777: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 778/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00778: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 779/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00779: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 780/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00780: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 781/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00781: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 782/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00782: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 783/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00783: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 784/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00784: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 785/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00785: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 786/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00786: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 787/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00787: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 788/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00788: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 789/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00789: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 790/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00790: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 791/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00791: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 792/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00792: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 793/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00793: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 794/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00794: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 795/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00795: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 796/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00796: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 797/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00797: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 798/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00798: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 799/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00799: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 800/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00800: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 801/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00801: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 802/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00802: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 803/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00803: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 804/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00804: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 805/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00805: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 806/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00806: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 807/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00807: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 808/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00808: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 809/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00809: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 810/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00810: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 811/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00811: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 812/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00812: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 813/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00813: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 814/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00814: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 815/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00815: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 816/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00816: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 817/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00817: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 818/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00818: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 819/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00819: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 820/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00820: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 821/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00821: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 822/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00822: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 823/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00823: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 824/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00824: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 825/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00825: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 826/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00826: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 827/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00827: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 828/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00828: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 829/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00829: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 830/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00830: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 831/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00831: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 832/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00832: val_loss improved from 0.01652 to 0.01650, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 833/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00833: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 834/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00834: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 835/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00835: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 836/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00836: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 837/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00837: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 838/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00838: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 839/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00839: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 840/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00840: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 841/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00841: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 842/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00842: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 843/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00843: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 844/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00844: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 845/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00845: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 846/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00846: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 847/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00847: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 848/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00848: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 849/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00849: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 850/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00850: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 851/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00851: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 852/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00852: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0167 - val_loss: 0.0166\n",
      "Epoch 853/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00853: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 854/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00854: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 855/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00855: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 856/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00856: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 857/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00857: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 858/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00858: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 859/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00859: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 860/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00860: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 861/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00861: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 862/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00862: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 863/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00863: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 864/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00864: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 865/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00865: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 866/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00866: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 867/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00867: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 868/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00868: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 869/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00869: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 870/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00870: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 871/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00871: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 872/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00872: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 873/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00873: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 874/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00874: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 875/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00875: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 876/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00876: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 877/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00877: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 878/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00878: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 879/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00879: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 880/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00880: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 881/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00881: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 882/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00882: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 883/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00883: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 884/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00884: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 885/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00885: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 886/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00886: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 887/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00887: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 888/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00888: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 889/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00889: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 890/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00890: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 891/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00891: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 892/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00892: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 893/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00893: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 894/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00894: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 895/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00895: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 896/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00896: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 897/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00897: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 898/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00898: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 899/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00899: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 900/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00900: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 901/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00901: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 902/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00902: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 903/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00903: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 904/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00904: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 905/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00905: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 906/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00906: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 907/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00907: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 908/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00908: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 909/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00909: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 910/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00910: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 911/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00911: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 912/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00912: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 913/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00913: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 914/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00914: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 915/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00915: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 916/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00916: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 917/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00917: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 918/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00918: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 919/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00919: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 920/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00920: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 921/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00921: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 922/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00922: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 923/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00923: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 924/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00924: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 925/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00925: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 926/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00926: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 927/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00927: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 928/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00928: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 929/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00929: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 930/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00930: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 931/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00931: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 932/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00932: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 933/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00933: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 934/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00934: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 935/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00935: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 936/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00936: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 937/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00937: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 938/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00938: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 939/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00939: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 940/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00940: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 941/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00941: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 942/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00942: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 943/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00943: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 944/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00944: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 945/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00945: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 946/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00946: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 947/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00947: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 948/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00948: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 949/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00949: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 950/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00950: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0179\n",
      "Epoch 951/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00951: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 952/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00952: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 953/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00953: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 954/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00954: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 955/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00955: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 956/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00956: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 957/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00957: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 958/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00958: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 959/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00959: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 960/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00960: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 961/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00961: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0172\n",
      "Epoch 962/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00962: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 963/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00963: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 964/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00964: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 965/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00965: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 966/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00966: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 967/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00967: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 968/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00968: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 969/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00969: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 970/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00970: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 971/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00971: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 972/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00972: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 973/1000\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00973: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 974/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00974: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 975/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00975: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 976/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00976: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 977/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00977: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 978/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00978: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0170\n",
      "Epoch 979/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00979: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 980/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00980: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 981/1000\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00981: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 982/1000\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00982: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 983/1000\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00983: val_loss did not improve from 0.01650\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 984/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00984: val_loss improved from 0.01650 to 0.01649, saving model to ../models/51_1split_dense.h5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 985/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00985: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 986/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00986: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 987/1000\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00987: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 988/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00988: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 989/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00989: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 990/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00990: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0169\n",
      "Epoch 991/1000\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00991: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 992/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00992: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 993/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00993: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0171\n",
      "Epoch 994/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00994: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0165\n",
      "Epoch 995/1000\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00995: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n",
      "Epoch 996/1000\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00996: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 997/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00997: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 998/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00998: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 999/1000\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00999: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0167\n",
      "Epoch 1000/1000\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 01000: val_loss did not improve from 0.01649\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0166 - val_loss: 0.0168\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "model = keras.models.load_model('../models/51_1split_dense.h5')\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('../models/51_1split_dense.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, x_train, validation_data=(x_test, x_test), epochs=1000, batch_size=32, callbacks=[mc])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('No-Split training')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.grid(b=True, which='major', color='#666666', linestyle='-')\n",
    "plt.savefig('../img/51_PLOTS/DenseConv1split_loss_2k_epochs.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f7e3c6a07f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 22 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "n = 7\n",
    "plt.figure(figsize=(15, 5))\n",
    "axs = []\n",
    "for i in range(n):\n",
    "    \n",
    "    # predict\n",
    "    x = x_test[i]\n",
    "    z = encoder(x.reshape(1,28,28,1))\n",
    "    y = decoder(z)\n",
    "    \n",
    "    axs.append(plt.subplot(3, n, i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape(28,28), vmin=0, vmax=1)\n",
    "    \n",
    "    axs.append(plt.subplot(3, n, n+i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(tf.reshape(z, (4,4)), vmin=0, vmax=1)\n",
    "    \n",
    "    axs.append(plt.subplot(3, n, 2*n+i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(tf.reshape(y, (28,28)), vmin=0, vmax=1)\n",
    "plt.colorbar(ax=axs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
