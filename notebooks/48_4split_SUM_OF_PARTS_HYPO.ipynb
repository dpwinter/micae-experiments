{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 1split (without dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        160       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          8256      \n",
      "_________________________________________________________________\n",
      "pt_conv1 (Conv2D)            (None, 4, 4, 1)           257       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "latent (Activation)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "Decoder (Model)              (None, 28, 28, 1)         30089     \n",
      "=================================================================\n",
      "Total params: 38,762\n",
      "Trainable params: 38,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# encoder\n",
    "enc_input = Input(shape=(28,28,1), name='enc_input')\n",
    "x  = Conv2D(filters=32, kernel_size=(2,2), strides=(2,2), activation='relu', padding='same', name='conv1')(enc_input)\n",
    "x  = Conv2D(filters=64, kernel_size=(2,2), strides=(2,2), activation='relu', padding='same', name='conv2')(x)\n",
    "x  = Conv2D(filters=1, kernel_size=(2,2), strides=(2,2), activation='relu', padding='same', name='pt_conv1')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "latent = Activation('sigmoid', name='latent')(x)\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Dense(units=7*7*8, activation='relu', name='dense')(dec_input)\n",
    "x = Reshape(target_shape=(7,7,8), name='reshape')(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv1')(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv2')(x)\n",
    "dec_output = Conv2DTranspose(filters=1, kernel_size=3, padding='same', name='pt_conv')(x)\n",
    "\n",
    "encoder = Model(enc_input, latent, name=\"Encoder\")\n",
    "decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "model = Model(encoder.input, decoder(encoder.output))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0381\n",
      "Epoch 00001: val_loss improved from inf to 0.02857, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0380 - val_loss: 0.0286\n",
      "Epoch 2/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0268\n",
      "Epoch 00002: val_loss improved from 0.02857 to 0.02464, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0268 - val_loss: 0.0246\n",
      "Epoch 3/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0243\n",
      "Epoch 00003: val_loss improved from 0.02464 to 0.02318, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0243 - val_loss: 0.0232\n",
      "Epoch 4/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0229\n",
      "Epoch 00004: val_loss improved from 0.02318 to 0.02201, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0229 - val_loss: 0.0220\n",
      "Epoch 5/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0221\n",
      "Epoch 00005: val_loss improved from 0.02201 to 0.02119, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0221 - val_loss: 0.0212\n",
      "Epoch 6/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0214\n",
      "Epoch 00006: val_loss improved from 0.02119 to 0.02067, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 7/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0209\n",
      "Epoch 00007: val_loss improved from 0.02067 to 0.02041, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0209 - val_loss: 0.0204\n",
      "Epoch 8/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0206\n",
      "Epoch 00008: val_loss improved from 0.02041 to 0.02014, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0206 - val_loss: 0.0201\n",
      "Epoch 9/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0202\n",
      "Epoch 00009: val_loss improved from 0.02014 to 0.01979, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0202 - val_loss: 0.0198\n",
      "Epoch 10/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0199\n",
      "Epoch 00010: val_loss improved from 0.01979 to 0.01946, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0199 - val_loss: 0.0195\n",
      "Epoch 11/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0197\n",
      "Epoch 00011: val_loss improved from 0.01946 to 0.01922, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0197 - val_loss: 0.0192\n",
      "Epoch 12/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0195\n",
      "Epoch 00012: val_loss improved from 0.01922 to 0.01920, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0195 - val_loss: 0.0192\n",
      "Epoch 13/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0193\n",
      "Epoch 00013: val_loss improved from 0.01920 to 0.01877, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0193 - val_loss: 0.0188\n",
      "Epoch 14/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0192\n",
      "Epoch 00014: val_loss did not improve from 0.01877\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0192 - val_loss: 0.0188\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 00015: val_loss improved from 0.01877 to 0.01874, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0190 - val_loss: 0.0187\n",
      "Epoch 16/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0188\n",
      "Epoch 00016: val_loss improved from 0.01874 to 0.01856, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0188 - val_loss: 0.0186\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0187\n",
      "Epoch 00017: val_loss improved from 0.01856 to 0.01818, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0187 - val_loss: 0.0182\n",
      "Epoch 18/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0186\n",
      "Epoch 00018: val_loss did not improve from 0.01818\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0185 - val_loss: 0.0183\n",
      "Epoch 19/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0185\n",
      "Epoch 00019: val_loss improved from 0.01818 to 0.01800, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 20/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0183\n",
      "Epoch 00020: val_loss improved from 0.01800 to 0.01791, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0183 - val_loss: 0.0179\n",
      "Epoch 21/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0182\n",
      "Epoch 00021: val_loss improved from 0.01791 to 0.01779, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0182 - val_loss: 0.0178\n",
      "Epoch 22/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0181\n",
      "Epoch 00022: val_loss did not improve from 0.01779\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0181 - val_loss: 0.0178\n",
      "Epoch 23/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0181\n",
      "Epoch 00023: val_loss did not improve from 0.01779\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0181 - val_loss: 0.0179\n",
      "Epoch 24/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0180\n",
      "Epoch 00024: val_loss improved from 0.01779 to 0.01757, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0180 - val_loss: 0.0176\n",
      "Epoch 25/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0179\n",
      "Epoch 00025: val_loss did not improve from 0.01757\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0179 - val_loss: 0.0177\n",
      "Epoch 26/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0178\n",
      "Epoch 00026: val_loss did not improve from 0.01757\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0179 - val_loss: 0.0185\n",
      "Epoch 27/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0177\n",
      "Epoch 00027: val_loss improved from 0.01757 to 0.01739, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0177 - val_loss: 0.0174\n",
      "Epoch 28/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0177\n",
      "Epoch 00028: val_loss did not improve from 0.01739\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 29/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0176\n",
      "Epoch 00029: val_loss improved from 0.01739 to 0.01719, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 30/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0176\n",
      "Epoch 00030: val_loss did not improve from 0.01719\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 31/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0175\n",
      "Epoch 00031: val_loss did not improve from 0.01719\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0175 - val_loss: 0.0175\n",
      "Epoch 32/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0175\n",
      "Epoch 00032: val_loss did not improve from 0.01719\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0175 - val_loss: 0.0174\n",
      "Epoch 33/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0174\n",
      "Epoch 00033: val_loss improved from 0.01719 to 0.01712, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0174 - val_loss: 0.0171\n",
      "Epoch 34/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0174\n",
      "Epoch 00034: val_loss did not improve from 0.01712\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0174 - val_loss: 0.0174\n",
      "Epoch 35/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0173\n",
      "Epoch 00035: val_loss improved from 0.01712 to 0.01709, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0173 - val_loss: 0.0171\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0173\n",
      "Epoch 00036: val_loss did not improve from 0.01709\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 37/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00037: val_loss improved from 0.01709 to 0.01678, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0172 - val_loss: 0.0168\n",
      "Epoch 38/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00038: val_loss did not improve from 0.01678\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0172 - val_loss: 0.0170\n",
      "Epoch 39/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00039: val_loss did not improve from 0.01678\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 40/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00040: val_loss improved from 0.01678 to 0.01676, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0171 - val_loss: 0.0168\n",
      "Epoch 41/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00041: val_loss did not improve from 0.01676\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 42/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0170\n",
      "Epoch 00042: val_loss improved from 0.01676 to 0.01660, saving model to ../models/48_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0170 - val_loss: 0.0166\n",
      "Epoch 43/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0170\n",
      "Epoch 00043: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0170 - val_loss: 0.0170\n",
      "Epoch 44/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00044: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00045: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0169 - val_loss: 0.0167\n",
      "Epoch 46/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00046: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0169 - val_loss: 0.0166\n",
      "Epoch 47/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00047: val_loss did not improve from 0.01660\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 00047: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('../models/48_1split.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, x_train, validation_data=(x_test, x_test), epochs=100, batch_size=32, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 4split with free weights (without dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class dataGen(keras.utils.Sequence):\n",
    "    def __init__(self, xs, batch_size=32):\n",
    "        self.batch_size=batch_size\n",
    "        self.xs = xs\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.xs)/self.batch_size)\n",
    "    def split(self, im, nrows=14, ncols=14):\n",
    "        r, h = im.shape[:-1] # exclude channel dim\n",
    "        return (im.reshape(h//nrows,nrows,-1,ncols).swapaxes(1,2).reshape(-1,nrows,ncols,1))\n",
    "    def __getitem__(self, i):\n",
    "        batch = self.xs[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        x1s, x2s, x3s, x4s = [], [], [], []\n",
    "        for x in batch:\n",
    "            x1, x2, x3, x4 = self.split(x)\n",
    "            x1s.append(x1)\n",
    "            x2s.append(x2)\n",
    "            x3s.append(x3)\n",
    "            x4s.append(x4)\n",
    "        xs = [np.array(x1s), np.array(x2s), np.array(x3s), np.array(x4s)]\n",
    "        return xs, batch\n",
    "    def on_epoch_end(self):\n",
    "        self.xs = np.random.permutation(self.xs)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "train_data = dataGen(x_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "test_data = dataGen(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 7, 7, 32)     320         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 7, 7, 32)     320         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 7, 7, 32)     320         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 7, 7, 32)     320         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 4, 4, 64)     18496       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 4, 4, 64)     18496       conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 4, 4, 64)     18496       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 4, 64)     18496       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 2, 2, 1)      577         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 2, 2, 1)      577         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 2, 2, 1)      577         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 2, 2, 1)      577         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 4)            0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 4)            0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 4)            0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 4)            0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "latent_concat (Concatenate)     (None, 16)           0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (Model)                 (None, 28, 28, 1)    19410       latent_concat[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 96,982\n",
      "Trainable params: 96,982\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, Activation\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "\n",
    "# encoder\n",
    "latents = []\n",
    "enc_inputs = [Input(shape=(14,14,1)) for _ in range(4)]\n",
    "for i in range(4):\n",
    "    conv1 = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same')\n",
    "    conv2 = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same')\n",
    "    pt_conv = Conv2D(filters=1, kernel_size=(3,3), strides=(1,1), activation='sigmoid', padding='valid')\n",
    "    latent = Flatten()\n",
    "    \n",
    "    enc_out = latent(pt_conv(conv2(conv1(enc_inputs[i]))))\n",
    "    latents.append(enc_out)\n",
    "latent_concat = keras.layers.concatenate(latents, name='latent_concat')\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "# x = Dense(units=7*7*8, activation='relu', name='dense')(dec_input)\n",
    "# x = Reshape(target_shape=(7,7,8), name='reshape')(x)\n",
    "x = Reshape(target_shape=(4,4,1))(dec_input)\n",
    "x = Conv2DTranspose(filters=1, kernel_size=(4,4), strides=(1,1), activation='relu', padding='valid')(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv1')(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv2')(x)\n",
    "dec_output = Conv2DTranspose(filters=1, kernel_size=3, padding='same', name='pt_conv')(x)\n",
    "\n",
    "encoders = [Model(enc_inputs[i], latents[i], name='Encoder_%d'%i) for i in range(4)]\n",
    "decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "model = Model(enc_inputs, decoder(latent_concat))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0559 - val_loss: 0.0488\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0458 - val_loss: 0.0441\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0428 - val_loss: 0.0413\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0412 - val_loss: 0.0401\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0401 - val_loss: 0.0394\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0392 - val_loss: 0.0388\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0386 - val_loss: 0.0378\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.0380 - val_loss: 0.0372\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0374 - val_loss: 0.0367\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0368 - val_loss: 0.0365\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0363 - val_loss: 0.0359\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0357 - val_loss: 0.0353\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0352 - val_loss: 0.0350\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0349 - val_loss: 0.0343\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0345 - val_loss: 0.0341\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.0343 - val_loss: 0.0340\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0340 - val_loss: 0.0340\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0338 - val_loss: 0.0336\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0336 - val_loss: 0.0336\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0334 - val_loss: 0.0331\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0332 - val_loss: 0.0336\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0330 - val_loss: 0.0327\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0329 - val_loss: 0.0327\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0327 - val_loss: 0.0327\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0326 - val_loss: 0.0323\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0325 - val_loss: 0.0323\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0323 - val_loss: 0.0323\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0322 - val_loss: 0.0320\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0321 - val_loss: 0.0317\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0320 - val_loss: 0.0317\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0319 - val_loss: 0.0321\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0318 - val_loss: 0.0329\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0317 - val_loss: 0.0313\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0316 - val_loss: 0.0315\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0315 - val_loss: 0.0313\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.0314 - val_loss: 0.0314\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0314 - val_loss: 0.0312\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0313 - val_loss: 0.0310\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0312 - val_loss: 0.0310\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.0311 - val_loss: 0.0311\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0310 - val_loss: 0.0308\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0310 - val_loss: 0.0308\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.0309 - val_loss: 0.0311\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0309 - val_loss: 0.0308\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0308 - val_loss: 0.0314\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0307 - val_loss: 0.0326\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0307 - val_loss: 0.0308\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0306 - val_loss: 0.0305\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0306 - val_loss: 0.0309\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.0305 - val_loss: 0.0304\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, validation_data=(test_data), epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 4split with tied weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 14, 14, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 7, 7, 32)     320         input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 4, 4, 64)     18496       conv1[0][0]                      \n",
      "                                                                 conv1[1][0]                      \n",
      "                                                                 conv1[2][0]                      \n",
      "                                                                 conv1[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pt_conv (Conv2D)                (None, 2, 2, 1)      577         conv2[0][0]                      \n",
      "                                                                 conv2[1][0]                      \n",
      "                                                                 conv2[2][0]                      \n",
      "                                                                 conv2[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4)            0           pt_conv[0][0]                    \n",
      "                                                                 pt_conv[1][0]                    \n",
      "                                                                 pt_conv[2][0]                    \n",
      "                                                                 pt_conv[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 4)            0           flatten[0][0]                    \n",
      "                                                                 flatten[1][0]                    \n",
      "                                                                 flatten[2][0]                    \n",
      "                                                                 flatten[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "latent_concat (Concatenate)     (None, 16)           0           activation[0][0]                 \n",
      "                                                                 activation[1][0]                 \n",
      "                                                                 activation[2][0]                 \n",
      "                                                                 activation[3][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Decoder (Model)                 (None, 28, 28, 1)    30089       latent_concat[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 49,482\n",
      "Trainable params: 49,482\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, Activation\n",
    "from keras.utils import plot_model\n",
    "import keras\n",
    "\n",
    "# shared layers\n",
    "enc_inputs = [Input(shape=(14,14,1)) for _ in range(4)]\n",
    "conv1 = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv1')\n",
    "conv2 = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv2')\n",
    "pt_conv = Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='pt_conv')\n",
    "flatten = Flatten(name='flatten')\n",
    "latent = Activation('sigmoid')\n",
    "\n",
    "# encoder\n",
    "latents = []\n",
    "for i in range(4):\n",
    "    enc_out = latent(flatten(pt_conv(conv2(conv1(enc_inputs[i])))))\n",
    "    latents.append(enc_out)\n",
    "latent_concat = keras.layers.concatenate(latents, name='latent_concat')\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Dense(units=7*7*8, activation='relu', name='dense')(dec_input)\n",
    "x = Reshape(target_shape=(7,7,8), name='reshape')(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv1')(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv2')(x)\n",
    "dec_output = Conv2DTranspose(filters=1, kernel_size=3, padding='same', name='pt_conv')(x)\n",
    "\n",
    "encoders = [Model(enc_inputs[i], latents[i], name='Encoder_%d'%i) for i in range(4)]\n",
    "decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "model = Model(enc_inputs, decoder(latent_concat))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0375\n",
      "Epoch 00001: val_loss improved from inf to 0.02818, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0375 - val_loss: 0.0282\n",
      "Epoch 2/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0256\n",
      "Epoch 00002: val_loss improved from 0.02818 to 0.02341, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0256 - val_loss: 0.0234\n",
      "Epoch 3/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0230\n",
      "Epoch 00003: val_loss improved from 0.02341 to 0.02149, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0230 - val_loss: 0.0215\n",
      "Epoch 4/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0216\n",
      "Epoch 00004: val_loss improved from 0.02149 to 0.02049, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0216 - val_loss: 0.0205\n",
      "Epoch 5/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0207\n",
      "Epoch 00005: val_loss improved from 0.02049 to 0.01999, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0207 - val_loss: 0.0200\n",
      "Epoch 6/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0200\n",
      "Epoch 00006: val_loss improved from 0.01999 to 0.01965, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0200 - val_loss: 0.0196\n",
      "Epoch 7/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0195\n",
      "Epoch 00007: val_loss improved from 0.01965 to 0.01883, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0196 - val_loss: 0.0188\n",
      "Epoch 8/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0191\n",
      "Epoch 00008: val_loss did not improve from 0.01883\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0191 - val_loss: 0.0190\n",
      "Epoch 9/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0188\n",
      "Epoch 00009: val_loss improved from 0.01883 to 0.01840, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0188 - val_loss: 0.0184\n",
      "Epoch 10/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0185\n",
      "Epoch 00010: val_loss improved from 0.01840 to 0.01817, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0185 - val_loss: 0.0182\n",
      "Epoch 11/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0183\n",
      "Epoch 00011: val_loss improved from 0.01817 to 0.01767, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0183 - val_loss: 0.0177\n",
      "Epoch 12/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0181\n",
      "Epoch 00012: val_loss did not improve from 0.01767\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0181 - val_loss: 0.0179\n",
      "Epoch 13/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0179\n",
      "Epoch 00013: val_loss did not improve from 0.01767\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0179 - val_loss: 0.0178\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0177\n",
      "Epoch 00014: val_loss improved from 0.01767 to 0.01723, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0177 - val_loss: 0.0172\n",
      "Epoch 15/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0175\n",
      "Epoch 00015: val_loss improved from 0.01723 to 0.01706, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0175 - val_loss: 0.0171\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0174\n",
      "Epoch 00016: val_loss improved from 0.01706 to 0.01683, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0174 - val_loss: 0.0168\n",
      "Epoch 17/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0173\n",
      "Epoch 00017: val_loss did not improve from 0.01683\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0173 - val_loss: 0.0171\n",
      "Epoch 18/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0171\n",
      "Epoch 00018: val_loss improved from 0.01683 to 0.01656, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0171 - val_loss: 0.0166\n",
      "Epoch 19/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0170\n",
      "Epoch 00019: val_loss improved from 0.01656 to 0.01652, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0171 - val_loss: 0.0165\n",
      "Epoch 20/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00020: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0169 - val_loss: 0.0170\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00021: val_loss did not improve from 0.01652\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00022: val_loss improved from 0.01652 to 0.01644, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0168 - val_loss: 0.0164\n",
      "Epoch 23/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0167\n",
      "Epoch 00023: val_loss did not improve from 0.01644\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0167 - val_loss: 0.0170\n",
      "Epoch 24/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00024: val_loss improved from 0.01644 to 0.01617, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0166 - val_loss: 0.0162\n",
      "Epoch 25/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00025: val_loss improved from 0.01617 to 0.01616, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0165 - val_loss: 0.0162\n",
      "Epoch 26/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00026: val_loss did not improve from 0.01616\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 27/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0164\n",
      "Epoch 00027: val_loss improved from 0.01616 to 0.01606, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0164 - val_loss: 0.0161\n",
      "Epoch 28/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0164\n",
      "Epoch 00028: val_loss did not improve from 0.01606\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0164 - val_loss: 0.0163\n",
      "Epoch 29/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0163\n",
      "Epoch 00029: val_loss did not improve from 0.01606\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 30/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0163\n",
      "Epoch 00030: val_loss improved from 0.01606 to 0.01578, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0163 - val_loss: 0.0158\n",
      "Epoch 31/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00031: val_loss did not improve from 0.01578\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0162 - val_loss: 0.0158\n",
      "Epoch 32/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00032: val_loss improved from 0.01578 to 0.01571, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0161 - val_loss: 0.0157\n",
      "Epoch 33/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00033: val_loss did not improve from 0.01571\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0161 - val_loss: 0.0158\n",
      "Epoch 34/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00034: val_loss improved from 0.01571 to 0.01561, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0160 - val_loss: 0.0156\n",
      "Epoch 35/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00035: val_loss did not improve from 0.01561\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0160 - val_loss: 0.0157\n",
      "Epoch 36/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00036: val_loss improved from 0.01561 to 0.01560, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0159 - val_loss: 0.0156\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00037: val_loss improved from 0.01560 to 0.01548, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0159 - val_loss: 0.0155\n",
      "Epoch 38/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00038: val_loss did not improve from 0.01548\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0159 - val_loss: 0.0155\n",
      "Epoch 39/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0158\n",
      "Epoch 00039: val_loss improved from 0.01548 to 0.01528, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0158 - val_loss: 0.0153\n",
      "Epoch 40/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0158\n",
      "Epoch 00040: val_loss did not improve from 0.01528\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0158 - val_loss: 0.0156\n",
      "Epoch 41/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00041: val_loss did not improve from 0.01528\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 42/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00042: val_loss did not improve from 0.01528\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0157 - val_loss: 0.0155\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00043: val_loss improved from 0.01528 to 0.01524, saving model to ../models/48_4split_shared.h5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0157 - val_loss: 0.0152\n",
      "Epoch 44/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00044: val_loss did not improve from 0.01524\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0157 - val_loss: 0.0156\n",
      "Epoch 45/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00045: val_loss did not improve from 0.01524\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 46/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00046: val_loss did not improve from 0.01524\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0156 - val_loss: 0.0155\n",
      "Epoch 47/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00047: val_loss did not improve from 0.01524\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0156 - val_loss: 0.0152\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00048: val_loss did not improve from 0.01524\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0155 - val_loss: 0.0153\n",
      "Epoch 00048: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('../models/48_4split_shared.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(train_data, validation_data=(test_data), epochs=100, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare classification 1-split vs. 4-split(shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. 1-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "pt_conv1 (Conv2D)            (None, 4, 4, 1)           577       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "latent (Activation)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "Decoder (Model)              (None, 10)                46154     \n",
      "=================================================================\n",
      "Total params: 65,547\n",
      "Trainable params: 65,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# encoder\n",
    "enc_input = Input(shape=(28,28,1), name='enc_input')\n",
    "x  = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv1')(enc_input)\n",
    "x  = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv2')(x)\n",
    "x  = Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='pt_conv1')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "latent = Activation('sigmoid', name='latent')(x)\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Dense(256, activation='relu')(dec_input)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "class_out = Dense(10, activation='softmax', name='classifier_out')(x)\n",
    "# x = Dense(units=7*7*8, activation='relu', name='dense')(dec_input)\n",
    "# x = Reshape(target_shape=(7,7,8), name='reshape')(x)\n",
    "# x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv1')(x)\n",
    "# x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv2')(x)\n",
    "# dec_output = Conv2DTranspose(filters=1, kernel_size=3, padding='same', name='pt_conv')(x)\n",
    "\n",
    "encoder = Model(enc_input, latent, name=\"Encoder\")\n",
    "decoder = Model(dec_input, class_out, name=\"Decoder\")\n",
    "model = Model(encoder.input, decoder(encoder.output))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1849/1875 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.8035\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.92260, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5979 - accuracy: 0.8051 - val_loss: 0.2588 - val_accuracy: 0.9226\n",
      "Epoch 2/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9270\n",
      "Epoch 00002: val_accuracy improved from 0.92260 to 0.93090, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2386 - accuracy: 0.9270 - val_loss: 0.2248 - val_accuracy: 0.9309\n",
      "Epoch 3/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.9429\n",
      "Epoch 00003: val_accuracy improved from 0.93090 to 0.95490, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1886 - accuracy: 0.9429 - val_loss: 0.1485 - val_accuracy: 0.9549\n",
      "Epoch 4/100\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9501\n",
      "Epoch 00004: val_accuracy improved from 0.95490 to 0.95790, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1602 - accuracy: 0.9500 - val_loss: 0.1409 - val_accuracy: 0.9579\n",
      "Epoch 5/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9567\n",
      "Epoch 00005: val_accuracy improved from 0.95790 to 0.96010, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1397 - accuracy: 0.9568 - val_loss: 0.1279 - val_accuracy: 0.9601\n",
      "Epoch 6/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9604\n",
      "Epoch 00006: val_accuracy did not improve from 0.96010\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1256 - accuracy: 0.9604 - val_loss: 0.1273 - val_accuracy: 0.9587\n",
      "Epoch 7/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9636\n",
      "Epoch 00007: val_accuracy did not improve from 0.96010\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1150 - accuracy: 0.9635 - val_loss: 0.1414 - val_accuracy: 0.9579\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1041 - accuracy: 0.9672\n",
      "Epoch 00008: val_accuracy did not improve from 0.96010\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1041 - accuracy: 0.9672 - val_loss: 0.1321 - val_accuracy: 0.9590\n",
      "Epoch 9/100\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.0977 - accuracy: 0.9693\n",
      "Epoch 00009: val_accuracy improved from 0.96010 to 0.96660, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0976 - accuracy: 0.9693 - val_loss: 0.1139 - val_accuracy: 0.9666\n",
      "Epoch 10/100\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9715\n",
      "Epoch 00010: val_accuracy improved from 0.96660 to 0.96740, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0922 - accuracy: 0.9714 - val_loss: 0.1120 - val_accuracy: 0.9674\n",
      "Epoch 11/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9731\n",
      "Epoch 00011: val_accuracy improved from 0.96740 to 0.96910, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0865 - accuracy: 0.9731 - val_loss: 0.1018 - val_accuracy: 0.9691\n",
      "Epoch 12/100\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 0.0821 - accuracy: 0.9739\n",
      "Epoch 00012: val_accuracy did not improve from 0.96910\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0825 - accuracy: 0.9737 - val_loss: 0.1030 - val_accuracy: 0.9680\n",
      "Epoch 13/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9751\n",
      "Epoch 00013: val_accuracy improved from 0.96910 to 0.97010, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0786 - accuracy: 0.9752 - val_loss: 0.0979 - val_accuracy: 0.9701\n",
      "Epoch 14/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9772\n",
      "Epoch 00014: val_accuracy did not improve from 0.97010\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0744 - accuracy: 0.9772 - val_loss: 0.1046 - val_accuracy: 0.9680\n",
      "Epoch 15/100\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9780\n",
      "Epoch 00015: val_accuracy did not improve from 0.97010\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0705 - accuracy: 0.9779 - val_loss: 0.1183 - val_accuracy: 0.9656\n",
      "Epoch 16/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9787\n",
      "Epoch 00016: val_accuracy improved from 0.97010 to 0.97090, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0667 - accuracy: 0.9786 - val_loss: 0.1011 - val_accuracy: 0.9709\n",
      "Epoch 17/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9796\n",
      "Epoch 00017: val_accuracy improved from 0.97090 to 0.97290, saving model to ../models/48_1split_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0641 - accuracy: 0.9797 - val_loss: 0.0925 - val_accuracy: 0.9729\n",
      "Epoch 18/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0604 - accuracy: 0.9806\n",
      "Epoch 00018: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0603 - accuracy: 0.9807 - val_loss: 0.1011 - val_accuracy: 0.9706\n",
      "Epoch 19/100\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.0579 - accuracy: 0.9814\n",
      "Epoch 00019: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0579 - accuracy: 0.9814 - val_loss: 0.0996 - val_accuracy: 0.9683\n",
      "Epoch 20/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9815\n",
      "Epoch 00020: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0580 - accuracy: 0.9815 - val_loss: 0.0989 - val_accuracy: 0.9702\n",
      "Epoch 21/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0554 - accuracy: 0.9814\n",
      "Epoch 00021: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0554 - accuracy: 0.9814 - val_loss: 0.1155 - val_accuracy: 0.9675\n",
      "Epoch 22/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0520 - accuracy: 0.9833\n",
      "Epoch 00022: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0520 - accuracy: 0.9833 - val_loss: 0.0978 - val_accuracy: 0.9713\n",
      "Epoch 23/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0506 - accuracy: 0.9834\n",
      "Epoch 00023: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0508 - accuracy: 0.9833 - val_loss: 0.1084 - val_accuracy: 0.9699\n",
      "Epoch 24/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0491 - accuracy: 0.9839\n",
      "Epoch 00024: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0491 - accuracy: 0.9839 - val_loss: 0.1057 - val_accuracy: 0.9690\n",
      "Epoch 25/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 0.9850\n",
      "Epoch 00025: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0466 - accuracy: 0.9850 - val_loss: 0.1081 - val_accuracy: 0.9691\n",
      "Epoch 26/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9853\n",
      "Epoch 00026: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0443 - accuracy: 0.9852 - val_loss: 0.1122 - val_accuracy: 0.9699\n",
      "Epoch 27/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9858\n",
      "Epoch 00027: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0451 - accuracy: 0.9858 - val_loss: 0.1141 - val_accuracy: 0.9703\n",
      "Epoch 28/100\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9867\n",
      "Epoch 00028: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0415 - accuracy: 0.9867 - val_loss: 0.1306 - val_accuracy: 0.9661\n",
      "Epoch 29/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9858\n",
      "Epoch 00029: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0432 - accuracy: 0.9858 - val_loss: 0.1110 - val_accuracy: 0.9694\n",
      "Epoch 30/100\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9868\n",
      "Epoch 00030: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0406 - accuracy: 0.9868 - val_loss: 0.1349 - val_accuracy: 0.9649\n",
      "Epoch 31/100\n",
      "1859/1875 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9876\n",
      "Epoch 00031: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0382 - accuracy: 0.9876 - val_loss: 0.1290 - val_accuracy: 0.9656\n",
      "Epoch 32/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9873\n",
      "Epoch 00032: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0390 - accuracy: 0.9873 - val_loss: 0.1098 - val_accuracy: 0.9710\n",
      "Epoch 33/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0391 - accuracy: 0.9872\n",
      "Epoch 00033: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0389 - accuracy: 0.9873 - val_loss: 0.1215 - val_accuracy: 0.9692\n",
      "Epoch 34/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0369 - accuracy: 0.9881\n",
      "Epoch 00034: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0369 - accuracy: 0.9880 - val_loss: 0.1289 - val_accuracy: 0.9680\n",
      "Epoch 35/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9883\n",
      "Epoch 00035: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0355 - accuracy: 0.9883 - val_loss: 0.1190 - val_accuracy: 0.9704\n",
      "Epoch 36/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9882\n",
      "Epoch 00036: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.1147 - val_accuracy: 0.9717\n",
      "Epoch 37/100\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9896\n",
      "Epoch 00037: val_accuracy did not improve from 0.97290\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0324 - accuracy: 0.9897 - val_loss: 0.1322 - val_accuracy: 0.9678\n",
      "Epoch 00037: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('../models/48_1split_class.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=32, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 4\n",
    "output_shape = (28, 28, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "latent_dim = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "#         self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "#         self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "#         y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "#         y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "#         y_regr = self.regress(z ,training=training)\n",
    "        return y_class\n",
    "#         return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Activation('sigmoid')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1859/1875 [============================>.] - ETA: 0s - loss: 0.8004 - accuracy: 0.7392\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89220, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.7969 - accuracy: 0.7403 - val_loss: 0.3509 - val_accuracy: 0.8922\n",
      "Epoch 2/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.3223 - accuracy: 0.8990\n",
      "Epoch 00002: val_accuracy improved from 0.89220 to 0.92590, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3219 - accuracy: 0.8990 - val_loss: 0.2477 - val_accuracy: 0.9259\n",
      "Epoch 3/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.2423 - accuracy: 0.9243\n",
      "Epoch 00003: val_accuracy improved from 0.92590 to 0.93060, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2413 - accuracy: 0.9245 - val_loss: 0.2219 - val_accuracy: 0.9306\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2038 - accuracy: 0.9367\n",
      "Epoch 00004: val_accuracy improved from 0.93060 to 0.94470, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2038 - accuracy: 0.9367 - val_loss: 0.1766 - val_accuracy: 0.9447\n",
      "Epoch 5/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9448\n",
      "Epoch 00005: val_accuracy improved from 0.94470 to 0.94740, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1770 - accuracy: 0.9449 - val_loss: 0.1694 - val_accuracy: 0.9474\n",
      "Epoch 6/100\n",
      "1852/1875 [============================>.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9496\n",
      "Epoch 00006: val_accuracy did not improve from 0.94740\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1621 - accuracy: 0.9497 - val_loss: 0.1763 - val_accuracy: 0.9440\n",
      "Epoch 7/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9545\n",
      "Epoch 00007: val_accuracy improved from 0.94740 to 0.95360, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1462 - accuracy: 0.9545 - val_loss: 0.1435 - val_accuracy: 0.9536\n",
      "Epoch 8/100\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.1399 - accuracy: 0.9555\n",
      "Epoch 00008: val_accuracy did not improve from 0.95360\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1399 - accuracy: 0.9555 - val_loss: 0.1716 - val_accuracy: 0.9466\n",
      "Epoch 9/100\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9594\n",
      "Epoch 00009: val_accuracy improved from 0.95360 to 0.96210, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1303 - accuracy: 0.9592 - val_loss: 0.1321 - val_accuracy: 0.9621\n",
      "Epoch 10/100\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9621\n",
      "Epoch 00010: val_accuracy did not improve from 0.96210\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1195 - accuracy: 0.9620 - val_loss: 0.1414 - val_accuracy: 0.9565\n",
      "Epoch 11/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9630\n",
      "Epoch 00011: val_accuracy did not improve from 0.96210\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1161 - accuracy: 0.9630 - val_loss: 0.1247 - val_accuracy: 0.9605\n",
      "Epoch 12/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9647\n",
      "Epoch 00012: val_accuracy did not improve from 0.96210\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1110 - accuracy: 0.9647 - val_loss: 0.1270 - val_accuracy: 0.9591\n",
      "Epoch 13/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.1067 - accuracy: 0.9665\n",
      "Epoch 00013: val_accuracy did not improve from 0.96210\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1070 - accuracy: 0.9664 - val_loss: 0.1366 - val_accuracy: 0.9574\n",
      "Epoch 14/100\n",
      "1851/1875 [============================>.] - ETA: 0s - loss: 0.1025 - accuracy: 0.9672\n",
      "Epoch 00014: val_accuracy did not improve from 0.96210\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1029 - accuracy: 0.9671 - val_loss: 0.1484 - val_accuracy: 0.9555\n",
      "Epoch 15/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0976 - accuracy: 0.9688\n",
      "Epoch 00015: val_accuracy improved from 0.96210 to 0.96540, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0978 - accuracy: 0.9688 - val_loss: 0.1066 - val_accuracy: 0.9654\n",
      "Epoch 16/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9698\n",
      "Epoch 00016: val_accuracy did not improve from 0.96540\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0947 - accuracy: 0.9699 - val_loss: 0.1293 - val_accuracy: 0.9621\n",
      "Epoch 17/100\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9717\n",
      "Epoch 00017: val_accuracy did not improve from 0.96540\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0902 - accuracy: 0.9718 - val_loss: 0.1237 - val_accuracy: 0.9627\n",
      "Epoch 18/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9712\n",
      "Epoch 00018: val_accuracy did not improve from 0.96540\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0883 - accuracy: 0.9711 - val_loss: 0.1167 - val_accuracy: 0.9650\n",
      "Epoch 19/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9719\n",
      "Epoch 00019: val_accuracy did not improve from 0.96540\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0861 - accuracy: 0.9720 - val_loss: 0.1299 - val_accuracy: 0.9619\n",
      "Epoch 20/100\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9744\n",
      "Epoch 00020: val_accuracy did not improve from 0.96540\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0794 - accuracy: 0.9744 - val_loss: 0.1229 - val_accuracy: 0.9633\n",
      "Epoch 21/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0802 - accuracy: 0.9741\n",
      "Epoch 00021: val_accuracy improved from 0.96540 to 0.96810, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0801 - accuracy: 0.9741 - val_loss: 0.1114 - val_accuracy: 0.9681\n",
      "Epoch 22/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0778 - accuracy: 0.9753\n",
      "Epoch 00022: val_accuracy improved from 0.96810 to 0.96840, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0776 - accuracy: 0.9754 - val_loss: 0.1117 - val_accuracy: 0.9684\n",
      "Epoch 23/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0784 - accuracy: 0.9742\n",
      "Epoch 00023: val_accuracy did not improve from 0.96840\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0784 - accuracy: 0.9742 - val_loss: 0.1196 - val_accuracy: 0.9643\n",
      "Epoch 24/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9769\n",
      "Epoch 00024: val_accuracy did not improve from 0.96840\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0719 - accuracy: 0.9769 - val_loss: 0.1135 - val_accuracy: 0.9666\n",
      "Epoch 25/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0713 - accuracy: 0.9771\n",
      "Epoch 00025: val_accuracy improved from 0.96840 to 0.96860, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0713 - accuracy: 0.9770 - val_loss: 0.1062 - val_accuracy: 0.9686\n",
      "Epoch 26/100\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.0687 - accuracy: 0.9778\n",
      "Epoch 00026: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0687 - accuracy: 0.9778 - val_loss: 0.1139 - val_accuracy: 0.9662\n",
      "Epoch 27/100\n",
      "1859/1875 [============================>.] - ETA: 0s - loss: 0.0670 - accuracy: 0.9785\n",
      "Epoch 00027: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0670 - accuracy: 0.9786 - val_loss: 0.1200 - val_accuracy: 0.9669\n",
      "Epoch 28/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9789\n",
      "Epoch 00028: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0652 - accuracy: 0.9790 - val_loss: 0.1399 - val_accuracy: 0.9597\n",
      "Epoch 29/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0628 - accuracy: 0.9795\n",
      "Epoch 00029: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0626 - accuracy: 0.9795 - val_loss: 0.1269 - val_accuracy: 0.9651\n",
      "Epoch 30/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9790\n",
      "Epoch 00030: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0626 - accuracy: 0.9790 - val_loss: 0.1275 - val_accuracy: 0.9653\n",
      "Epoch 31/100\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0594 - accuracy: 0.9800\n",
      "Epoch 00031: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.1200 - val_accuracy: 0.9673\n",
      "Epoch 32/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0607 - accuracy: 0.9801\n",
      "Epoch 00032: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0608 - accuracy: 0.9800 - val_loss: 0.1193 - val_accuracy: 0.9677\n",
      "Epoch 33/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9811\n",
      "Epoch 00033: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0563 - accuracy: 0.9811 - val_loss: 0.1233 - val_accuracy: 0.9679\n",
      "Epoch 34/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0580 - accuracy: 0.9814\n",
      "Epoch 00034: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0579 - accuracy: 0.9814 - val_loss: 0.1267 - val_accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0555 - accuracy: 0.9814\n",
      "Epoch 00035: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0555 - accuracy: 0.9814 - val_loss: 0.1274 - val_accuracy: 0.9646\n",
      "Epoch 36/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9822\n",
      "Epoch 00036: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0528 - accuracy: 0.9821 - val_loss: 0.1219 - val_accuracy: 0.9675\n",
      "Epoch 37/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9821\n",
      "Epoch 00037: val_accuracy did not improve from 0.96860\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0539 - accuracy: 0.9822 - val_loss: 0.1344 - val_accuracy: 0.9620\n",
      "Epoch 38/100\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.0502 - accuracy: 0.9834\n",
      "Epoch 00038: val_accuracy improved from 0.96860 to 0.96940, saving model to ../models/48_4split_shared_class.h5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0504 - accuracy: 0.9834 - val_loss: 0.1100 - val_accuracy: 0.9694\n",
      "Epoch 39/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9835\n",
      "Epoch 00039: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0508 - accuracy: 0.9835 - val_loss: 0.1137 - val_accuracy: 0.9680\n",
      "Epoch 40/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9840\n",
      "Epoch 00040: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0484 - accuracy: 0.9840 - val_loss: 0.1269 - val_accuracy: 0.9690\n",
      "Epoch 41/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0468 - accuracy: 0.9840\n",
      "Epoch 00041: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0467 - accuracy: 0.9841 - val_loss: 0.1333 - val_accuracy: 0.9663\n",
      "Epoch 42/100\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.0480 - accuracy: 0.9841\n",
      "Epoch 00042: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0481 - accuracy: 0.9841 - val_loss: 0.1291 - val_accuracy: 0.9683\n",
      "Epoch 43/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9843\n",
      "Epoch 00043: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0455 - accuracy: 0.9844 - val_loss: 0.1342 - val_accuracy: 0.9659\n",
      "Epoch 44/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0467 - accuracy: 0.9842\n",
      "Epoch 00044: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0469 - accuracy: 0.9841 - val_loss: 0.1525 - val_accuracy: 0.9626\n",
      "Epoch 45/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9854\n",
      "Epoch 00045: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0432 - accuracy: 0.9854 - val_loss: 0.1533 - val_accuracy: 0.9637\n",
      "Epoch 46/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0452 - accuracy: 0.9850\n",
      "Epoch 00046: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0452 - accuracy: 0.9850 - val_loss: 0.1377 - val_accuracy: 0.9655\n",
      "Epoch 47/100\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.0422 - accuracy: 0.9859\n",
      "Epoch 00047: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0422 - accuracy: 0.9859 - val_loss: 0.1603 - val_accuracy: 0.9597\n",
      "Epoch 48/100\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9855\n",
      "Epoch 00048: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0424 - accuracy: 0.9855 - val_loss: 0.1309 - val_accuracy: 0.9677\n",
      "Epoch 49/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9858\n",
      "Epoch 00049: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9858 - val_loss: 0.1639 - val_accuracy: 0.9604\n",
      "Epoch 50/100\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0380 - accuracy: 0.9872\n",
      "Epoch 00050: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0381 - accuracy: 0.9872 - val_loss: 0.1378 - val_accuracy: 0.9644\n",
      "Epoch 51/100\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.0394 - accuracy: 0.9865\n",
      "Epoch 00051: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.1322 - val_accuracy: 0.9679\n",
      "Epoch 52/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0395 - accuracy: 0.9868\n",
      "Epoch 00052: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0396 - accuracy: 0.9867 - val_loss: 0.1393 - val_accuracy: 0.9663\n",
      "Epoch 53/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0392 - accuracy: 0.9868\n",
      "Epoch 00053: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0391 - accuracy: 0.9868 - val_loss: 0.1519 - val_accuracy: 0.9640\n",
      "Epoch 54/100\n",
      "1852/1875 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9873\n",
      "Epoch 00054: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0375 - accuracy: 0.9873 - val_loss: 0.1442 - val_accuracy: 0.9659\n",
      "Epoch 55/100\n",
      "1860/1875 [============================>.] - ETA: 0s - loss: 0.0377 - accuracy: 0.9870\n",
      "Epoch 00055: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0379 - accuracy: 0.9869 - val_loss: 0.1492 - val_accuracy: 0.9646\n",
      "Epoch 56/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 0.9876\n",
      "Epoch 00056: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0377 - accuracy: 0.9877 - val_loss: 0.1347 - val_accuracy: 0.9676\n",
      "Epoch 57/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0374 - accuracy: 0.9873\n",
      "Epoch 00057: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0374 - accuracy: 0.9872 - val_loss: 0.1568 - val_accuracy: 0.9633\n",
      "Epoch 58/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9883\n",
      "Epoch 00058: val_accuracy did not improve from 0.96940\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0354 - accuracy: 0.9883 - val_loss: 0.1508 - val_accuracy: 0.9665\n",
      "Epoch 00058: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('../models/48_4split_shared_class.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train_split, y_train, validation_data=(x_test_split, y_test), epochs=100, batch_size=32, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f9e46c74100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 22 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "n = 7\n",
    "# n = 1\n",
    "plt.figure(figsize=(15, 5))\n",
    "axs = []\n",
    "for (xs, ys), i in zip(test_data, range(n)):\n",
    "#     axs.append(plt.subplot(3, n, i+1))\n",
    "    ax = plt.subplot(3, n, i+1)\n",
    "    for j in range(4):\n",
    "        r = int(j/2)\n",
    "        c = j%2\n",
    "        ins = ax.inset_axes([c*0.55,0.5-r*0.5,0.5,0.5])\n",
    "        ins.axis('off')\n",
    "        ins.imshow(xs[j][0], vmin=0, vmax=1)\n",
    "    ax.axis('off')\n",
    "    axs.append(ax)\n",
    "        \n",
    "    z = [encoder(x[0].reshape(1,14,14,1)) for encoder, x in zip(encoders, xs)]\n",
    "    a = np.empty((4,4))\n",
    "    for j in range(4):\n",
    "        r = int(j/2)\n",
    "        c = j%2\n",
    "        a[r*2:(r+1)*2, c*2:(c+1)*2] = tf.reshape(z[j], (2,2))\n",
    "    axs.append(plt.subplot(3, n, n+i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(a, vmin=0, vmax=1)\n",
    "    \n",
    "    y = model.predict(xs)\n",
    "    axs.append(plt.subplot(3, n, 2*n+i+1))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(tf.reshape(y[0], (28,28)), vmin=0, vmax=1)\n",
    "plt.colorbar(ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f9dd59e3970>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# model.load_weights('../weights/41_CAE4x4_epoch50_freeWeights_weights.h5')\n",
    "# n = 7\n",
    "n = 1\n",
    "# plt.figure(figsize=(15, 5))\n",
    "plt.figure(figsize=(6, 10))\n",
    "axs = []\n",
    "for (xs, ys), i in zip(test_data, range(n)):\n",
    "#     axs.append(plt.subplot(3, n, i+1))\n",
    "#     ax = plt.subplot(3, n, i+1)\n",
    "    ax = plt.subplot(2, 1, 1)\n",
    "    for j in range(4):\n",
    "        r = int(j/2)\n",
    "        c = j%2\n",
    "#         ins = ax.inset_axes([c*0.55,0.5-r*0.5,0.5,0.5])\n",
    "        ins = ax.inset_axes([c*0.55,0.5-r*0.55,0.5,0.5])\n",
    "        ins.axis('off')\n",
    "        ins.imshow(xs[0][0], vmin=0, vmax=1)\n",
    "    ax.axis('off')\n",
    "    axs.append(ax)\n",
    "        \n",
    "    z = [encoder(xs[0][0].reshape(1,14,14,1)) for encoder, x in zip(encoders, xs)]\n",
    "    a = np.empty((4,4))\n",
    "    for j in range(4):\n",
    "        r = int(j/2)\n",
    "        c = j%2\n",
    "        a[r*2:(r+1)*2, c*2:(c+1)*2] = tf.reshape(z[j], (2,2))\n",
    "#     axs.append(plt.subplot(3, n, n+i+1))\n",
    "    axs.append(plt.subplot(2, 1, 2))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(a, vmin=0, vmax=1)\n",
    "    \n",
    "#     y = model.predict(xs)\n",
    "#     axs.append(plt.subplot(3, n, 2*n+i+1))\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(tf.reshape(y[0], (28,28)), vmin=0, vmax=1)\n",
    "plt.colorbar(ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "def embed_and_translate(data, n_width, n_height):\n",
    "    ndata = np.zeros((len(data), n_width, n_height, 1), dtype='float32')\n",
    "    translations = np.empty((len(data), 2), dtype='float32')\n",
    "    width, height = data.shape[1], data.shape[2]\n",
    "    for i in range(len(data)):\n",
    "        x = np.random.randint(n_width-width)\n",
    "        y = np.random.randint(n_height-height)\n",
    "        ndata[i][x:x+width, y:y+height] = data[i] # rows, cols = height, width\n",
    "        translations[i][0] = x+(width//2)\n",
    "        translations[i][1] = y+(height//2)\n",
    "    return ndata, translations\n",
    "            \n",
    "n_splits = 16\n",
    "output_shape = (56, 56, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "latent_dim = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_augmented, y_train_regr = embed_and_translate(x_train, output_shape[0], output_shape[1])\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train_augmented], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_augmented, y_test_regr = embed_and_translate(x_test, output_shape[0], output_shape[1])\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test_augmented], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-split reco performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "        self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "        self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "        y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "        y_regr = self.regress(z ,training=training)\n",
    "        return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Activation('sigmoid')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 32)          320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 1)           577       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 19,393\n",
      "Trainable params: 19,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1568)              101920    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 64)        4672      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 56, 56, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 56, 56, 1)         289       \n",
      "_________________________________________________________________\n",
      "decoder_out (Activation)     (None, 56, 56, 1)         0         \n",
      "=================================================================\n",
      "Total params: 125,345\n",
      "Trainable params: 125,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "losses = {\n",
    "    'decoder_out' : 'mse',\n",
    "    'classifier_out' : 'categorical_crossentropy',\n",
    "    'regressor_out' : 'mean_absolute_percentage_error'\n",
    "}\n",
    "loss_weights = {\n",
    "    'decoder_out' : 1.0,\n",
    "    'classifier_out' : 0.0,\n",
    "    'regressor_out' : 0.0\n",
    "}\n",
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss=losses, loss_weights=loss_weights, optimizer='adam', metrics=['accuracy'])\n",
    "print(model.encoder.summary())\n",
    "print(model.decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0190 - classifier_out_loss: 2.3239 - decoder_out_loss: 0.0190 - regressor_out_loss: 99.7312 - classifier_out_accuracy: 0.0967 - decoder_out_accuracy: 0.9474 - regressor_out_accuracy: 0.4986 - val_loss: 0.0469 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0469 - val_regressor_out_loss: 99.7600 - val_classifier_out_accuracy: 0.0966 - val_decoder_out_accuracy: 0.7826 - val_regressor_out_accuracy: 0.5017\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0109 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0109 - regressor_out_loss: 99.7597 - classifier_out_accuracy: 0.0956 - decoder_out_accuracy: 0.9469 - regressor_out_accuracy: 0.4970 - val_loss: 0.0405 - val_classifier_out_loss: 2.3193 - val_decoder_out_loss: 0.0405 - val_regressor_out_loss: 99.7590 - val_classifier_out_accuracy: 0.0969 - val_decoder_out_accuracy: 0.7944 - val_regressor_out_accuracy: 0.4957\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0099 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0099 - regressor_out_loss: 99.7589 - classifier_out_accuracy: 0.0962 - decoder_out_accuracy: 0.9484 - regressor_out_accuracy: 0.4963 - val_loss: 0.0380 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0380 - val_regressor_out_loss: 99.7566 - val_classifier_out_accuracy: 0.0979 - val_decoder_out_accuracy: 0.7920 - val_regressor_out_accuracy: 0.4990\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0094 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0094 - regressor_out_loss: 99.7581 - classifier_out_accuracy: 0.0962 - decoder_out_accuracy: 0.9490 - regressor_out_accuracy: 0.4951 - val_loss: 0.0365 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0365 - val_regressor_out_loss: 99.7559 - val_classifier_out_accuracy: 0.0979 - val_decoder_out_accuracy: 0.7946 - val_regressor_out_accuracy: 0.4980\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0090 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0090 - regressor_out_loss: 99.7574 - classifier_out_accuracy: 0.0963 - decoder_out_accuracy: 0.9494 - regressor_out_accuracy: 0.4942 - val_loss: 0.0356 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0356 - val_regressor_out_loss: 99.7555 - val_classifier_out_accuracy: 0.0991 - val_decoder_out_accuracy: 0.7992 - val_regressor_out_accuracy: 0.4980\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0088 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0088 - regressor_out_loss: 99.7564 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9496 - regressor_out_accuracy: 0.4932 - val_loss: 0.0343 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0343 - val_regressor_out_loss: 99.7561 - val_classifier_out_accuracy: 0.0986 - val_decoder_out_accuracy: 0.7987 - val_regressor_out_accuracy: 0.4951\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0085 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0085 - regressor_out_loss: 99.7560 - classifier_out_accuracy: 0.0963 - decoder_out_accuracy: 0.9498 - regressor_out_accuracy: 0.4923 - val_loss: 0.0331 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0331 - val_regressor_out_loss: 99.7533 - val_classifier_out_accuracy: 0.0974 - val_decoder_out_accuracy: 0.7998 - val_regressor_out_accuracy: 0.4937\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0083 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0083 - regressor_out_loss: 99.7554 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9500 - regressor_out_accuracy: 0.4912 - val_loss: 0.0325 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0325 - val_regressor_out_loss: 99.7543 - val_classifier_out_accuracy: 0.0979 - val_decoder_out_accuracy: 0.7989 - val_regressor_out_accuracy: 0.4943\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0081 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0081 - regressor_out_loss: 99.7550 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9501 - regressor_out_accuracy: 0.4907 - val_loss: 0.0323 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0323 - val_regressor_out_loss: 99.7505 - val_classifier_out_accuracy: 0.0973 - val_decoder_out_accuracy: 0.8008 - val_regressor_out_accuracy: 0.4927\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0080 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0080 - regressor_out_loss: 99.7551 - classifier_out_accuracy: 0.0964 - decoder_out_accuracy: 0.9503 - regressor_out_accuracy: 0.4904 - val_loss: 0.0318 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0318 - val_regressor_out_loss: 99.7554 - val_classifier_out_accuracy: 0.0980 - val_decoder_out_accuracy: 0.8010 - val_regressor_out_accuracy: 0.4948\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0078 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0078 - regressor_out_loss: 99.7553 - classifier_out_accuracy: 0.0962 - decoder_out_accuracy: 0.9503 - regressor_out_accuracy: 0.4902 - val_loss: 0.0311 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0311 - val_regressor_out_loss: 99.7539 - val_classifier_out_accuracy: 0.0976 - val_decoder_out_accuracy: 0.8020 - val_regressor_out_accuracy: 0.4933\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0077 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0077 - regressor_out_loss: 99.7549 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9504 - regressor_out_accuracy: 0.4898 - val_loss: 0.0312 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0312 - val_regressor_out_loss: 99.7565 - val_classifier_out_accuracy: 0.0973 - val_decoder_out_accuracy: 0.8011 - val_regressor_out_accuracy: 0.4913\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0076 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0076 - regressor_out_loss: 99.7546 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9505 - regressor_out_accuracy: 0.4896 - val_loss: 0.0301 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0301 - val_regressor_out_loss: 99.7533 - val_classifier_out_accuracy: 0.0973 - val_decoder_out_accuracy: 0.8008 - val_regressor_out_accuracy: 0.4914\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0075 - classifier_out_loss: 2.3198 - decoder_out_loss: 0.0075 - regressor_out_loss: 99.7551 - classifier_out_accuracy: 0.0964 - decoder_out_accuracy: 0.9506 - regressor_out_accuracy: 0.4896 - val_loss: 0.0301 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0301 - val_regressor_out_loss: 99.7542 - val_classifier_out_accuracy: 0.0982 - val_decoder_out_accuracy: 0.8001 - val_regressor_out_accuracy: 0.4919\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0074 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0074 - regressor_out_loss: 99.7550 - classifier_out_accuracy: 0.0963 - decoder_out_accuracy: 0.9506 - regressor_out_accuracy: 0.4893 - val_loss: 0.0296 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0296 - val_regressor_out_loss: 99.7536 - val_classifier_out_accuracy: 0.0977 - val_decoder_out_accuracy: 0.8015 - val_regressor_out_accuracy: 0.4921\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0074 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0074 - regressor_out_loss: 99.7544 - classifier_out_accuracy: 0.0964 - decoder_out_accuracy: 0.9507 - regressor_out_accuracy: 0.4895 - val_loss: 0.0291 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0291 - val_regressor_out_loss: 99.7530 - val_classifier_out_accuracy: 0.0980 - val_decoder_out_accuracy: 0.8037 - val_regressor_out_accuracy: 0.4922\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0073 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0073 - regressor_out_loss: 99.7543 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9507 - regressor_out_accuracy: 0.4893 - val_loss: 0.0291 - val_classifier_out_loss: 2.3195 - val_decoder_out_loss: 0.0291 - val_regressor_out_loss: 99.7490 - val_classifier_out_accuracy: 0.0975 - val_decoder_out_accuracy: 0.8026 - val_regressor_out_accuracy: 0.4885\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0072 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0072 - regressor_out_loss: 99.7542 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9508 - regressor_out_accuracy: 0.4891 - val_loss: 0.0288 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0288 - val_regressor_out_loss: 99.7539 - val_classifier_out_accuracy: 0.0975 - val_decoder_out_accuracy: 0.8012 - val_regressor_out_accuracy: 0.4927\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0072 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0072 - regressor_out_loss: 99.7540 - classifier_out_accuracy: 0.0962 - decoder_out_accuracy: 0.9508 - regressor_out_accuracy: 0.4891 - val_loss: 0.0285 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0285 - val_regressor_out_loss: 99.7491 - val_classifier_out_accuracy: 0.0969 - val_decoder_out_accuracy: 0.8034 - val_regressor_out_accuracy: 0.4907\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0071 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0071 - regressor_out_loss: 99.7543 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9509 - regressor_out_accuracy: 0.4892 - val_loss: 0.0284 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0284 - val_regressor_out_loss: 99.7526 - val_classifier_out_accuracy: 0.0975 - val_decoder_out_accuracy: 0.8034 - val_regressor_out_accuracy: 0.4920\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0071 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0071 - regressor_out_loss: 99.7543 - classifier_out_accuracy: 0.0962 - decoder_out_accuracy: 0.9509 - regressor_out_accuracy: 0.4890 - val_loss: 0.0278 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0278 - val_regressor_out_loss: 99.7515 - val_classifier_out_accuracy: 0.0971 - val_decoder_out_accuracy: 0.8029 - val_regressor_out_accuracy: 0.4922\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0070 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0070 - regressor_out_loss: 99.7543 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9509 - regressor_out_accuracy: 0.4891 - val_loss: 0.0280 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0280 - val_regressor_out_loss: 99.7531 - val_classifier_out_accuracy: 0.0975 - val_decoder_out_accuracy: 0.8029 - val_regressor_out_accuracy: 0.4914\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0070 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0070 - regressor_out_loss: 99.7539 - classifier_out_accuracy: 0.0964 - decoder_out_accuracy: 0.9509 - regressor_out_accuracy: 0.4890 - val_loss: 0.0279 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0279 - val_regressor_out_loss: 99.7497 - val_classifier_out_accuracy: 0.0969 - val_decoder_out_accuracy: 0.8029 - val_regressor_out_accuracy: 0.4899\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0069 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0069 - regressor_out_loss: 99.7539 - classifier_out_accuracy: 0.0964 - decoder_out_accuracy: 0.9510 - regressor_out_accuracy: 0.4890 - val_loss: 0.0277 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0277 - val_regressor_out_loss: 99.7531 - val_classifier_out_accuracy: 0.0971 - val_decoder_out_accuracy: 0.8032 - val_regressor_out_accuracy: 0.4922\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0069 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0069 - regressor_out_loss: 99.7541 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9510 - regressor_out_accuracy: 0.4890 - val_loss: 0.0283 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0283 - val_regressor_out_loss: 99.7530 - val_classifier_out_accuracy: 0.0967 - val_decoder_out_accuracy: 0.8042 - val_regressor_out_accuracy: 0.4925\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0068 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0068 - regressor_out_loss: 99.7536 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9510 - regressor_out_accuracy: 0.4888 - val_loss: 0.0275 - val_classifier_out_loss: 2.3193 - val_decoder_out_loss: 0.0275 - val_regressor_out_loss: 99.7510 - val_classifier_out_accuracy: 0.0968 - val_decoder_out_accuracy: 0.8039 - val_regressor_out_accuracy: 0.4914\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0068 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0068 - regressor_out_loss: 99.7538 - classifier_out_accuracy: 0.0963 - decoder_out_accuracy: 0.9510 - regressor_out_accuracy: 0.4888 - val_loss: 0.0272 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0272 - val_regressor_out_loss: 99.7509 - val_classifier_out_accuracy: 0.0964 - val_decoder_out_accuracy: 0.8036 - val_regressor_out_accuracy: 0.4909\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0068 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0068 - regressor_out_loss: 99.7537 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9511 - regressor_out_accuracy: 0.4889 - val_loss: 0.0267 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0267 - val_regressor_out_loss: 99.7520 - val_classifier_out_accuracy: 0.0966 - val_decoder_out_accuracy: 0.8038 - val_regressor_out_accuracy: 0.4918\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0068 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0068 - regressor_out_loss: 99.7538 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9511 - regressor_out_accuracy: 0.4886 - val_loss: 0.0273 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0273 - val_regressor_out_loss: 99.7534 - val_classifier_out_accuracy: 0.0975 - val_decoder_out_accuracy: 0.8036 - val_regressor_out_accuracy: 0.4919\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0067 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0067 - regressor_out_loss: 99.7534 - classifier_out_accuracy: 0.0967 - decoder_out_accuracy: 0.9511 - regressor_out_accuracy: 0.4884 - val_loss: 0.0267 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0267 - val_regressor_out_loss: 99.7522 - val_classifier_out_accuracy: 0.0975 - val_decoder_out_accuracy: 0.8034 - val_regressor_out_accuracy: 0.4904\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0067 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0067 - regressor_out_loss: 99.7538 - classifier_out_accuracy: 0.0967 - decoder_out_accuracy: 0.9511 - regressor_out_accuracy: 0.4886 - val_loss: 0.0267 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0267 - val_regressor_out_loss: 99.7550 - val_classifier_out_accuracy: 0.0970 - val_decoder_out_accuracy: 0.8038 - val_regressor_out_accuracy: 0.4916\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0067 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0067 - regressor_out_loss: 99.7539 - classifier_out_accuracy: 0.0968 - decoder_out_accuracy: 0.9511 - regressor_out_accuracy: 0.4893 - val_loss: 0.0266 - val_classifier_out_loss: 2.3194 - val_decoder_out_loss: 0.0266 - val_regressor_out_loss: 99.7510 - val_classifier_out_accuracy: 0.0971 - val_decoder_out_accuracy: 0.8036 - val_regressor_out_accuracy: 0.4889\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0066 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0066 - regressor_out_loss: 99.7538 - classifier_out_accuracy: 0.0969 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4889 - val_loss: 0.0268 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0268 - val_regressor_out_loss: 99.7545 - val_classifier_out_accuracy: 0.0971 - val_decoder_out_accuracy: 0.8029 - val_regressor_out_accuracy: 0.4931\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0066 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0066 - regressor_out_loss: 99.7533 - classifier_out_accuracy: 0.0970 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4887 - val_loss: 0.0266 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0266 - val_regressor_out_loss: 99.7541 - val_classifier_out_accuracy: 0.0970 - val_decoder_out_accuracy: 0.8046 - val_regressor_out_accuracy: 0.4923\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0066 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0066 - regressor_out_loss: 99.7541 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4888 - val_loss: 0.0266 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0266 - val_regressor_out_loss: 99.7516 - val_classifier_out_accuracy: 0.0964 - val_decoder_out_accuracy: 0.8044 - val_regressor_out_accuracy: 0.4899\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0065 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0065 - regressor_out_loss: 99.7543 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4892 - val_loss: 0.0259 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0259 - val_regressor_out_loss: 99.7502 - val_classifier_out_accuracy: 0.0967 - val_decoder_out_accuracy: 0.8040 - val_regressor_out_accuracy: 0.4908\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0065 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0065 - regressor_out_loss: 99.7547 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4889 - val_loss: 0.0262 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0262 - val_regressor_out_loss: 99.7536 - val_classifier_out_accuracy: 0.0958 - val_decoder_out_accuracy: 0.8049 - val_regressor_out_accuracy: 0.4919\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0065 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0065 - regressor_out_loss: 99.7544 - classifier_out_accuracy: 0.0969 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4884 - val_loss: 0.0262 - val_classifier_out_loss: 2.3193 - val_decoder_out_loss: 0.0262 - val_regressor_out_loss: 99.7516 - val_classifier_out_accuracy: 0.0965 - val_decoder_out_accuracy: 0.8046 - val_regressor_out_accuracy: 0.4894\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.0065 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0065 - regressor_out_loss: 99.7544 - classifier_out_accuracy: 0.0968 - decoder_out_accuracy: 0.9512 - regressor_out_accuracy: 0.4887 - val_loss: 0.0259 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0259 - val_regressor_out_loss: 99.7502 - val_classifier_out_accuracy: 0.0962 - val_decoder_out_accuracy: 0.8041 - val_regressor_out_accuracy: 0.4913\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0065 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0065 - regressor_out_loss: 99.7541 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4891 - val_loss: 0.0264 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0264 - val_regressor_out_loss: 99.7524 - val_classifier_out_accuracy: 0.0970 - val_decoder_out_accuracy: 0.8030 - val_regressor_out_accuracy: 0.4913\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0064 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0064 - regressor_out_loss: 99.7534 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4884 - val_loss: 0.0260 - val_classifier_out_loss: 2.3194 - val_decoder_out_loss: 0.0260 - val_regressor_out_loss: 99.7519 - val_classifier_out_accuracy: 0.0954 - val_decoder_out_accuracy: 0.8047 - val_regressor_out_accuracy: 0.4900\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0064 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0064 - regressor_out_loss: 99.7542 - classifier_out_accuracy: 0.0968 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4885 - val_loss: 0.0259 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0259 - val_regressor_out_loss: 99.7536 - val_classifier_out_accuracy: 0.0974 - val_decoder_out_accuracy: 0.8030 - val_regressor_out_accuracy: 0.4924\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0064 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0064 - regressor_out_loss: 99.7536 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4884 - val_loss: 0.0259 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0259 - val_regressor_out_loss: 99.7537 - val_classifier_out_accuracy: 0.0964 - val_decoder_out_accuracy: 0.8052 - val_regressor_out_accuracy: 0.4925\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0064 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0064 - regressor_out_loss: 99.7536 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4884 - val_loss: 0.0256 - val_classifier_out_loss: 2.3192 - val_decoder_out_loss: 0.0256 - val_regressor_out_loss: 99.7500 - val_classifier_out_accuracy: 0.0962 - val_decoder_out_accuracy: 0.8041 - val_regressor_out_accuracy: 0.4901\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0064 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0064 - regressor_out_loss: 99.7536 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4884 - val_loss: 0.0257 - val_classifier_out_loss: 2.3194 - val_decoder_out_loss: 0.0257 - val_regressor_out_loss: 99.7513 - val_classifier_out_accuracy: 0.0962 - val_decoder_out_accuracy: 0.8046 - val_regressor_out_accuracy: 0.4881\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0064 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0064 - regressor_out_loss: 99.7541 - classifier_out_accuracy: 0.0967 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4888 - val_loss: 0.0257 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0257 - val_regressor_out_loss: 99.7535 - val_classifier_out_accuracy: 0.0965 - val_decoder_out_accuracy: 0.8051 - val_regressor_out_accuracy: 0.4913\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0063 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0063 - regressor_out_loss: 99.7538 - classifier_out_accuracy: 0.0965 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4886 - val_loss: 0.0257 - val_classifier_out_loss: 2.3195 - val_decoder_out_loss: 0.0257 - val_regressor_out_loss: 99.7489 - val_classifier_out_accuracy: 0.0964 - val_decoder_out_accuracy: 0.8046 - val_regressor_out_accuracy: 0.4908\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0063 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0063 - regressor_out_loss: 99.7539 - classifier_out_accuracy: 0.0964 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4886 - val_loss: 0.0253 - val_classifier_out_loss: 2.3195 - val_decoder_out_loss: 0.0253 - val_regressor_out_loss: 99.7477 - val_classifier_out_accuracy: 0.0958 - val_decoder_out_accuracy: 0.8048 - val_regressor_out_accuracy: 0.4869\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0063 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0063 - regressor_out_loss: 99.7534 - classifier_out_accuracy: 0.0966 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4885 - val_loss: 0.0254 - val_classifier_out_loss: 2.3191 - val_decoder_out_loss: 0.0254 - val_regressor_out_loss: 99.7533 - val_classifier_out_accuracy: 0.0962 - val_decoder_out_accuracy: 0.8045 - val_regressor_out_accuracy: 0.4918\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0063 - classifier_out_loss: 2.3199 - decoder_out_loss: 0.0063 - regressor_out_loss: 99.7537 - classifier_out_accuracy: 0.0962 - decoder_out_accuracy: 0.9513 - regressor_out_accuracy: 0.4886 - val_loss: 0.0252 - val_classifier_out_loss: 2.3190 - val_decoder_out_loss: 0.0252 - val_regressor_out_loss: 99.7536 - val_classifier_out_accuracy: 0.0966 - val_decoder_out_accuracy: 0.8044 - val_regressor_out_accuracy: 0.4922\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n",
    "                    validation_data=(x_test_split, {'decoder_out': x_test, 'classifier_out': y_test, 'regressor_out':y_test_regr}),\n",
    "                    epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "n = 5\n",
    "plt.figure(figsize=(15, 7))\n",
    "axs = []\n",
    "for i in range(n):\n",
    "    \n",
    "    # predict\n",
    "    x = x_test_split[i].reshape(1,16,14,14,1)\n",
    "    z = model.encode(x, training=False)\n",
    "    y = model.decode(z, training=False)\n",
    "\n",
    "    a = np.empty((56,56))\n",
    "    ax = plt.subplot(3, n, i+1)\n",
    "    for j in range(16):\n",
    "        r = int(j/4)\n",
    "        c = j%4\n",
    "#         ins = ax.inset_axes([c*0.25,0.25-r*0.25,0.20,0.20])\n",
    "        ins = ax.inset_axes([0.05+c*0.25,0.75-r*0.25,0.20,0.25])\n",
    "        ins.axis('off')\n",
    "        ins.imshow(tf.reshape(x[0][j], (14, 14)), vmin=0, vmax=1)\n",
    "    ax.axis('off')\n",
    "    axs.append(ax)\n",
    "    \n",
    "#     ax = plt.subplot(3, n, i+1)\n",
    "#     plt.axis('off')\n",
    "#     axs.append(plt.imshow(x_test_augmented[i].reshape(56,56), vmin=0, vmax=1))\n",
    "    \n",
    "    a = np.empty((8,8))\n",
    "    for j in range(16):\n",
    "        r = int(j/4)\n",
    "        c = j%4\n",
    "        a[r*2:(r+1)*2, c*2:(c+1)*2] = tf.reshape(z[j], (2,2))\n",
    "    ax = plt.subplot(3, n, n+i+1) \n",
    "    plt.axis('off')\n",
    "    plt.imshow(a, vmin=0, vmax=1)\n",
    "    axs.append(ax)\n",
    "    \n",
    "    ax = plt.subplot(3, n, 2*n+i+1) \n",
    "    plt.axis('off')\n",
    "    plt.imshow(tf.reshape(y, (56,56)), vmin=0, vmax=1)\n",
    "    axs.append(ax)\n",
    "plt.colorbar(ax=axs)\n",
    "plt.savefig('../img/48_PLOTS/16split_reco_expl.png',  bbox_inches='tight', transparent=True, pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../weights/48_CAE4x16_epoch50_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more filters to decoder 8->16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "        self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "        self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "        y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "        y_regr = self.regress(z ,training=training)\n",
    "        return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Activation('sigmoid')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 16),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 16)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'decoder_out' : 'mse',\n",
    "    'classifier_out' : 'categorical_crossentropy',\n",
    "    'regressor_out' : 'mean_absolute_percentage_error'\n",
    "}\n",
    "loss_weights = {\n",
    "    'decoder_out' : 1.0,\n",
    "    'classifier_out' : 0.0,\n",
    "    'regressor_out' : 0.0\n",
    "}\n",
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss=losses, loss_weights=loss_weights, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0184 - classifier_out_loss: 2.3415 - decoder_out_loss: 0.0184 - regressor_out_loss: 100.7152 - val_loss: 0.0472 - val_classifier_out_loss: 2.3369 - val_decoder_out_loss: 0.0472 - val_regressor_out_loss: 100.6786\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0108 - classifier_out_loss: 2.3365 - decoder_out_loss: 0.0108 - regressor_out_loss: 100.6720 - val_loss: 0.0401 - val_classifier_out_loss: 2.3364 - val_decoder_out_loss: 0.0401 - val_regressor_out_loss: 100.6745\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0098 - classifier_out_loss: 2.3367 - decoder_out_loss: 0.0098 - regressor_out_loss: 100.6730 - val_loss: 0.0380 - val_classifier_out_loss: 2.3363 - val_decoder_out_loss: 0.0380 - val_regressor_out_loss: 100.6723\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0093 - classifier_out_loss: 2.3367 - decoder_out_loss: 0.0093 - regressor_out_loss: 100.6732 - val_loss: 0.0363 - val_classifier_out_loss: 2.3362 - val_decoder_out_loss: 0.0363 - val_regressor_out_loss: 100.6719\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0090 - classifier_out_loss: 2.3367 - decoder_out_loss: 0.0090 - regressor_out_loss: 100.6728 - val_loss: 0.0350 - val_classifier_out_loss: 2.3361 - val_decoder_out_loss: 0.0350 - val_regressor_out_loss: 100.6700\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0087 - classifier_out_loss: 2.3367 - decoder_out_loss: 0.0087 - regressor_out_loss: 100.6730 - val_loss: 0.0344 - val_classifier_out_loss: 2.3364 - val_decoder_out_loss: 0.0344 - val_regressor_out_loss: 100.6736\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0084 - classifier_out_loss: 2.3367 - decoder_out_loss: 0.0084 - regressor_out_loss: 100.6727 - val_loss: 0.0339 - val_classifier_out_loss: 2.3361 - val_decoder_out_loss: 0.0339 - val_regressor_out_loss: 100.6699\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0082 - classifier_out_loss: 2.3366 - decoder_out_loss: 0.0082 - regressor_out_loss: 100.6721 - val_loss: 0.0324 - val_classifier_out_loss: 2.3364 - val_decoder_out_loss: 0.0324 - val_regressor_out_loss: 100.6711\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0080 - classifier_out_loss: 2.3366 - decoder_out_loss: 0.0080 - regressor_out_loss: 100.6717 - val_loss: 0.0338 - val_classifier_out_loss: 2.3361 - val_decoder_out_loss: 0.0338 - val_regressor_out_loss: 100.6702\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0079 - classifier_out_loss: 2.3366 - decoder_out_loss: 0.0079 - regressor_out_loss: 100.6714 - val_loss: 0.0310 - val_classifier_out_loss: 2.3362 - val_decoder_out_loss: 0.0310 - val_regressor_out_loss: 100.6707\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0077 - classifier_out_loss: 2.3365 - decoder_out_loss: 0.0077 - regressor_out_loss: 100.6711 - val_loss: 0.0308 - val_classifier_out_loss: 2.3364 - val_decoder_out_loss: 0.0308 - val_regressor_out_loss: 100.6704\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0076 - classifier_out_loss: 2.3365 - decoder_out_loss: 0.0076 - regressor_out_loss: 100.6707 - val_loss: 0.0300 - val_classifier_out_loss: 2.3363 - val_decoder_out_loss: 0.0300 - val_regressor_out_loss: 100.6713\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0075 - classifier_out_loss: 2.3365 - decoder_out_loss: 0.0075 - regressor_out_loss: 100.6703 - val_loss: 0.0297 - val_classifier_out_loss: 2.3360 - val_decoder_out_loss: 0.0297 - val_regressor_out_loss: 100.6678\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0074 - classifier_out_loss: 2.3364 - decoder_out_loss: 0.0074 - regressor_out_loss: 100.6701 - val_loss: 0.0291 - val_classifier_out_loss: 2.3360 - val_decoder_out_loss: 0.0291 - val_regressor_out_loss: 100.6697\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0073 - classifier_out_loss: 2.3364 - decoder_out_loss: 0.0073 - regressor_out_loss: 100.6703 - val_loss: 0.0288 - val_classifier_out_loss: 2.3359 - val_decoder_out_loss: 0.0288 - val_regressor_out_loss: 100.6680\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0072 - classifier_out_loss: 2.3364 - decoder_out_loss: 0.0072 - regressor_out_loss: 100.6695 - val_loss: 0.0291 - val_classifier_out_loss: 2.3361 - val_decoder_out_loss: 0.0291 - val_regressor_out_loss: 100.6713\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0071 - classifier_out_loss: 2.3364 - decoder_out_loss: 0.0071 - regressor_out_loss: 100.6701 - val_loss: 0.0293 - val_classifier_out_loss: 2.3359 - val_decoder_out_loss: 0.0293 - val_regressor_out_loss: 100.6696\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0070 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0070 - regressor_out_loss: 100.6696 - val_loss: 0.0283 - val_classifier_out_loss: 2.3360 - val_decoder_out_loss: 0.0283 - val_regressor_out_loss: 100.6713\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0069 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0069 - regressor_out_loss: 100.6693 - val_loss: 0.0276 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0276 - val_regressor_out_loss: 100.6668\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0069 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0069 - regressor_out_loss: 100.6688 - val_loss: 0.0274 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0274 - val_regressor_out_loss: 100.6673\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0068 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0068 - regressor_out_loss: 100.6692 - val_loss: 0.0278 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0278 - val_regressor_out_loss: 100.6661\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0068 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0068 - regressor_out_loss: 100.6695 - val_loss: 0.0271 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0271 - val_regressor_out_loss: 100.6669\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0067 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0067 - regressor_out_loss: 100.6692 - val_loss: 0.0268 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0268 - val_regressor_out_loss: 100.6678\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0067 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0067 - regressor_out_loss: 100.6693 - val_loss: 0.0265 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0265 - val_regressor_out_loss: 100.6679\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0066 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0066 - regressor_out_loss: 100.6692 - val_loss: 0.0267 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0267 - val_regressor_out_loss: 100.6681\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0066 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0066 - regressor_out_loss: 100.6692 - val_loss: 0.0264 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0264 - val_regressor_out_loss: 100.6670\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0066 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0066 - regressor_out_loss: 100.6697 - val_loss: 0.0265 - val_classifier_out_loss: 2.3361 - val_decoder_out_loss: 0.0265 - val_regressor_out_loss: 100.6732\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0065 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0065 - regressor_out_loss: 100.6701 - val_loss: 0.0265 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0265 - val_regressor_out_loss: 100.6685\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0065 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0065 - regressor_out_loss: 100.6698 - val_loss: 0.0262 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0262 - val_regressor_out_loss: 100.6696\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0065 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0065 - regressor_out_loss: 100.6696 - val_loss: 0.0262 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0262 - val_regressor_out_loss: 100.6688\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0064 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0064 - regressor_out_loss: 100.6698 - val_loss: 0.0263 - val_classifier_out_loss: 2.3356 - val_decoder_out_loss: 0.0263 - val_regressor_out_loss: 100.6660\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0064 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0064 - regressor_out_loss: 100.6696 - val_loss: 0.0256 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0256 - val_regressor_out_loss: 100.6674\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0064 - classifier_out_loss: 2.3362 - decoder_out_loss: 0.0064 - regressor_out_loss: 100.6694 - val_loss: 0.0254 - val_classifier_out_loss: 2.3360 - val_decoder_out_loss: 0.0254 - val_regressor_out_loss: 100.6735\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0064 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0064 - regressor_out_loss: 100.6696 - val_loss: 0.0260 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0260 - val_regressor_out_loss: 100.6678\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0063 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0063 - regressor_out_loss: 100.6697 - val_loss: 0.0256 - val_classifier_out_loss: 2.3359 - val_decoder_out_loss: 0.0256 - val_regressor_out_loss: 100.6687\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0063 - classifier_out_loss: 2.3362 - decoder_out_loss: 0.0063 - regressor_out_loss: 100.6692 - val_loss: 0.0251 - val_classifier_out_loss: 2.3360 - val_decoder_out_loss: 0.0251 - val_regressor_out_loss: 100.6701\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0063 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0063 - regressor_out_loss: 100.6694 - val_loss: 0.0252 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0252 - val_regressor_out_loss: 100.6696\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0063 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0063 - regressor_out_loss: 100.6695 - val_loss: 0.0252 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0252 - val_regressor_out_loss: 100.6698\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0062 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0062 - regressor_out_loss: 100.6702 - val_loss: 0.0251 - val_classifier_out_loss: 2.3359 - val_decoder_out_loss: 0.0251 - val_regressor_out_loss: 100.6723\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0062 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0062 - regressor_out_loss: 100.6698 - val_loss: 0.0248 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0248 - val_regressor_out_loss: 100.6676\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0062 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0062 - regressor_out_loss: 100.6700 - val_loss: 0.0250 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0250 - val_regressor_out_loss: 100.6696\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0062 - classifier_out_loss: 2.3362 - decoder_out_loss: 0.0062 - regressor_out_loss: 100.6693 - val_loss: 0.0255 - val_classifier_out_loss: 2.3356 - val_decoder_out_loss: 0.0255 - val_regressor_out_loss: 100.6657\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0062 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0062 - regressor_out_loss: 100.6688 - val_loss: 0.0246 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0246 - val_regressor_out_loss: 100.6695\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0061 - classifier_out_loss: 2.3362 - decoder_out_loss: 0.0061 - regressor_out_loss: 100.6690 - val_loss: 0.0248 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0248 - val_regressor_out_loss: 100.6691\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0061 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0061 - regressor_out_loss: 100.6697 - val_loss: 0.0250 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0250 - val_regressor_out_loss: 100.6690\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0061 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0061 - regressor_out_loss: 100.6701 - val_loss: 0.0244 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0244 - val_regressor_out_loss: 100.6699\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0061 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0061 - regressor_out_loss: 100.6698 - val_loss: 0.0242 - val_classifier_out_loss: 2.3357 - val_decoder_out_loss: 0.0242 - val_regressor_out_loss: 100.6671\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0061 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0061 - regressor_out_loss: 100.6698 - val_loss: 0.0246 - val_classifier_out_loss: 2.3363 - val_decoder_out_loss: 0.0246 - val_regressor_out_loss: 100.6742\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0060 - classifier_out_loss: 2.3362 - decoder_out_loss: 0.0060 - regressor_out_loss: 100.6696 - val_loss: 0.0250 - val_classifier_out_loss: 2.3363 - val_decoder_out_loss: 0.0250 - val_regressor_out_loss: 100.6759\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0060 - classifier_out_loss: 2.3363 - decoder_out_loss: 0.0060 - regressor_out_loss: 100.6696 - val_loss: 0.0245 - val_classifier_out_loss: 2.3358 - val_decoder_out_loss: 0.0245 - val_regressor_out_loss: 100.6690\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n",
    "                    validation_data=(x_test_split, {'decoder_out': x_test, 'classifier_out': y_test, 'regressor_out':y_test_regr}),\n",
    "                    epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16-split without translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "def embed_and_translate(data, n_width, n_height):\n",
    "    ndata = np.zeros((len(data), n_width, n_height, 1), dtype='float32')\n",
    "    translations = np.empty((len(data), 2), dtype='float32')\n",
    "    width, height = data.shape[1], data.shape[2]\n",
    "    x, y = 0, 0\n",
    "    for i in range(len(data)):\n",
    "#         x = np.random.randint(n_width-width)\n",
    "#         y = np.random.randint(n_height-height)\n",
    "        ndata[i][x:x+width, y:y+height] = data[i] # rows, cols = height, width\n",
    "        translations[i][0] = x+(width//2)\n",
    "        translations[i][1] = y+(height//2)\n",
    "    return ndata, translations\n",
    "            \n",
    "n_splits = 16\n",
    "output_shape = (56, 56, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "latent_dim = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_augmented, y_train_regr = embed_and_translate(x_train, output_shape[0], output_shape[1])\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train_augmented], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_augmented, y_test_regr = embed_and_translate(x_test, output_shape[0], output_shape[1])\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test_augmented], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14. 14.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd43ddcf40>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_test_augmented[0].reshape(56,56))\n",
    "print(y_test_regr[0])\n",
    "im = tf.image.extract_glimpse(x_test_augmented[:1], (28,28), y_test_regr[:1], centered=False, normalized=False, noise='zero')\n",
    "plt.imshow(tf.reshape(im, (28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "        self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "        self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "        y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "        y_regr = self.regress(z ,training=training)\n",
    "        return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Activation('sigmoid')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'decoder_out' : 'mse',\n",
    "    'classifier_out' : 'categorical_crossentropy',\n",
    "    'regressor_out' : 'mean_absolute_percentage_error'\n",
    "}\n",
    "loss_weights = {\n",
    "    'decoder_out' : 1.0,\n",
    "    'classifier_out' : 0.0,\n",
    "    'regressor_out' : 0.0\n",
    "}\n",
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss=losses, loss_weights=loss_weights, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0134 - classifier_out_loss: 2.3239 - decoder_out_loss: 0.0134 - regressor_out_loss: 99.1152 - val_loss: 0.0303 - val_classifier_out_loss: 2.3220 - val_decoder_out_loss: 0.0303 - val_regressor_out_loss: 99.1840\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0069 - classifier_out_loss: 2.3220 - decoder_out_loss: 0.0069 - regressor_out_loss: 99.1926 - val_loss: 0.0251 - val_classifier_out_loss: 2.3228 - val_decoder_out_loss: 0.0251 - val_regressor_out_loss: 99.1993\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0060 - classifier_out_loss: 2.3228 - decoder_out_loss: 0.0060 - regressor_out_loss: 99.1944 - val_loss: 0.0226 - val_classifier_out_loss: 2.3236 - val_decoder_out_loss: 0.0226 - val_regressor_out_loss: 99.1997\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0055 - classifier_out_loss: 2.3232 - decoder_out_loss: 0.0055 - regressor_out_loss: 99.1888 - val_loss: 0.0207 - val_classifier_out_loss: 2.3236 - val_decoder_out_loss: 0.0207 - val_regressor_out_loss: 99.2012\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0052 - classifier_out_loss: 2.3233 - decoder_out_loss: 0.0052 - regressor_out_loss: 99.1824 - val_loss: 0.0196 - val_classifier_out_loss: 2.3237 - val_decoder_out_loss: 0.0196 - val_regressor_out_loss: 99.1832\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0049 - classifier_out_loss: 2.3232 - decoder_out_loss: 0.0049 - regressor_out_loss: 99.1812 - val_loss: 0.0187 - val_classifier_out_loss: 2.3236 - val_decoder_out_loss: 0.0187 - val_regressor_out_loss: 99.1806\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0048 - classifier_out_loss: 2.3232 - decoder_out_loss: 0.0048 - regressor_out_loss: 99.1796 - val_loss: 0.0186 - val_classifier_out_loss: 2.3236 - val_decoder_out_loss: 0.0186 - val_regressor_out_loss: 99.1824\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0046 - classifier_out_loss: 2.3231 - decoder_out_loss: 0.0046 - regressor_out_loss: 99.1803 - val_loss: 0.0178 - val_classifier_out_loss: 2.3236 - val_decoder_out_loss: 0.0178 - val_regressor_out_loss: 99.1760\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0045 - classifier_out_loss: 2.3231 - decoder_out_loss: 0.0045 - regressor_out_loss: 99.1808 - val_loss: 0.0173 - val_classifier_out_loss: 2.3236 - val_decoder_out_loss: 0.0173 - val_regressor_out_loss: 99.1901\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0044 - classifier_out_loss: 2.3230 - decoder_out_loss: 0.0044 - regressor_out_loss: 99.1809 - val_loss: 0.0171 - val_classifier_out_loss: 2.3235 - val_decoder_out_loss: 0.0171 - val_regressor_out_loss: 99.1837\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0043 - classifier_out_loss: 2.3230 - decoder_out_loss: 0.0043 - regressor_out_loss: 99.1815 - val_loss: 0.0165 - val_classifier_out_loss: 2.3234 - val_decoder_out_loss: 0.0165 - val_regressor_out_loss: 99.1714\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0042 - classifier_out_loss: 2.3229 - decoder_out_loss: 0.0042 - regressor_out_loss: 99.1820 - val_loss: 0.0164 - val_classifier_out_loss: 2.3234 - val_decoder_out_loss: 0.0164 - val_regressor_out_loss: 99.1928\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0042 - classifier_out_loss: 2.3229 - decoder_out_loss: 0.0042 - regressor_out_loss: 99.1829 - val_loss: 0.0162 - val_classifier_out_loss: 2.3235 - val_decoder_out_loss: 0.0162 - val_regressor_out_loss: 99.1921\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0041 - classifier_out_loss: 2.3228 - decoder_out_loss: 0.0041 - regressor_out_loss: 99.1838 - val_loss: 0.0164 - val_classifier_out_loss: 2.3234 - val_decoder_out_loss: 0.0164 - val_regressor_out_loss: 99.1883\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0041 - classifier_out_loss: 2.3228 - decoder_out_loss: 0.0041 - regressor_out_loss: 99.1840 - val_loss: 0.0159 - val_classifier_out_loss: 2.3235 - val_decoder_out_loss: 0.0159 - val_regressor_out_loss: 99.1841\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0040 - classifier_out_loss: 2.3227 - decoder_out_loss: 0.0040 - regressor_out_loss: 99.1840 - val_loss: 0.0157 - val_classifier_out_loss: 2.3233 - val_decoder_out_loss: 0.0157 - val_regressor_out_loss: 99.1879\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0040 - classifier_out_loss: 2.3227 - decoder_out_loss: 0.0040 - regressor_out_loss: 99.1853 - val_loss: 0.0156 - val_classifier_out_loss: 2.3232 - val_decoder_out_loss: 0.0156 - val_regressor_out_loss: 99.1868\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0040 - classifier_out_loss: 2.3227 - decoder_out_loss: 0.0040 - regressor_out_loss: 99.1863 - val_loss: 0.0152 - val_classifier_out_loss: 2.3232 - val_decoder_out_loss: 0.0152 - val_regressor_out_loss: 99.1850\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0039 - classifier_out_loss: 2.3227 - decoder_out_loss: 0.0039 - regressor_out_loss: 99.1864 - val_loss: 0.0152 - val_classifier_out_loss: 2.3230 - val_decoder_out_loss: 0.0152 - val_regressor_out_loss: 99.1937\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0039 - classifier_out_loss: 2.3226 - decoder_out_loss: 0.0039 - regressor_out_loss: 99.1866 - val_loss: 0.0152 - val_classifier_out_loss: 2.3232 - val_decoder_out_loss: 0.0152 - val_regressor_out_loss: 99.1880\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0039 - classifier_out_loss: 2.3226 - decoder_out_loss: 0.0039 - regressor_out_loss: 99.1871 - val_loss: 0.0151 - val_classifier_out_loss: 2.3232 - val_decoder_out_loss: 0.0151 - val_regressor_out_loss: 99.1854\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0038 - classifier_out_loss: 2.3225 - decoder_out_loss: 0.0038 - regressor_out_loss: 99.1885 - val_loss: 0.0153 - val_classifier_out_loss: 2.3228 - val_decoder_out_loss: 0.0153 - val_regressor_out_loss: 99.2013\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0038 - classifier_out_loss: 2.3225 - decoder_out_loss: 0.0038 - regressor_out_loss: 99.1897 - val_loss: 0.0149 - val_classifier_out_loss: 2.3231 - val_decoder_out_loss: 0.0149 - val_regressor_out_loss: 99.1918\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0038 - classifier_out_loss: 2.3225 - decoder_out_loss: 0.0038 - regressor_out_loss: 99.1895 - val_loss: 0.0149 - val_classifier_out_loss: 2.3231 - val_decoder_out_loss: 0.0149 - val_regressor_out_loss: 99.1917\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0038 - classifier_out_loss: 2.3225 - decoder_out_loss: 0.0038 - regressor_out_loss: 99.1894 - val_loss: 0.0147 - val_classifier_out_loss: 2.3228 - val_decoder_out_loss: 0.0147 - val_regressor_out_loss: 99.1906\n",
      "Epoch 26/50\n",
      " 121/1875 [>.............................] - ETA: 22s - loss: 0.0037 - classifier_out_loss: 2.3271 - decoder_out_loss: 0.0037 - regressor_out_loss: 99.1916"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9c8f2999461c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'decoder_out'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'classifier_out'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'regressor_out'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_test_regr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     epochs=50, batch_size=32)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2735\u001b[0m           *args, **kwargs)\n\u001b[1;32m   2736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2737\u001b[0;31m     \u001b[0mcache_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_cache_key\u001b[0;34m(self, args, kwargs, include_tensor_ranks_only)\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     return CacheKey(\n\u001b[0;32m-> 2635\u001b[0;31m         \u001b[0m_make_input_signature_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2636\u001b[0m         \u001b[0mdevice_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocation_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_cross_replica_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2637\u001b[0m         xla_context_id)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_make_input_signature_hashable\u001b[0;34m(elem, variable_map)\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;31m# TODO(slebedev): consider using nest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     return tuple(map(lambda e: _make_input_signature_hashable(e, variable_map),\n\u001b[0m\u001b[1;32m    106\u001b[0m                      elem))\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;31m# TODO(slebedev): consider using nest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     return tuple(map(lambda e: _make_input_signature_hashable(e, variable_map),\n\u001b[0m\u001b[1;32m    106\u001b[0m                      elem))\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n",
    "                    validation_data=(x_test_split, {'decoder_out': x_test, 'classifier_out': y_test, 'regressor_out':y_test_regr}),\n",
    "                    epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification on \"old\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 16)                50192     \n",
      "_________________________________________________________________\n",
      "Decoder (Model)              (None, 10)                46154     \n",
      "=================================================================\n",
      "Total params: 115,162\n",
      "Trainable params: 115,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# encoder\n",
    "enc_input = Input(shape=(28,28,1), name='enc_input')\n",
    "x  = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv1')(enc_input)\n",
    "x  = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv2')(x)\n",
    "# x  = Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='pt_conv1')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "latent = Dense(units=16, activation='sigmoid', name='latent')(x)\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Dense(256, activation='relu')(dec_input)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "class_out = Dense(10, activation='softmax', name='classifier_out')(x)\n",
    "# x = Dense(units=7*7*8, activation='relu', name='dense')(dec_input)\n",
    "# x = Reshape(target_shape=(7,7,8), name='reshape')(x)\n",
    "# x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv1')(x)\n",
    "# x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv2')(x)\n",
    "# dec_output = Conv2DTranspose(filters=1, kernel_size=3, padding='same', name='pt_conv')(x)\n",
    "\n",
    "encoder = Model(enc_input, latent, name=\"Encoder\")\n",
    "decoder = Model(dec_input, class_out, name=\"Decoder\")\n",
    "model = Model(encoder.input, decoder(encoder.output))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1857/1875 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.9191\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.96870, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2611 - accuracy: 0.9196 - val_loss: 0.0990 - val_accuracy: 0.9687\n",
      "Epoch 2/100\n",
      "1851/1875 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9745\n",
      "Epoch 00002: val_accuracy improved from 0.96870 to 0.97970, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0834 - accuracy: 0.9746 - val_loss: 0.0629 - val_accuracy: 0.9797\n",
      "Epoch 3/100\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.0591 - accuracy: 0.9819\n",
      "Epoch 00003: val_accuracy improved from 0.97970 to 0.98240, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0590 - accuracy: 0.9820 - val_loss: 0.0583 - val_accuracy: 0.9824\n",
      "Epoch 4/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0473 - accuracy: 0.9861\n",
      "Epoch 00004: val_accuracy did not improve from 0.98240\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0474 - accuracy: 0.9861 - val_loss: 0.0574 - val_accuracy: 0.9823\n",
      "Epoch 5/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0372 - accuracy: 0.9884\n",
      "Epoch 00005: val_accuracy improved from 0.98240 to 0.98390, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0374 - accuracy: 0.9884 - val_loss: 0.0490 - val_accuracy: 0.9839\n",
      "Epoch 6/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9902\n",
      "Epoch 00006: val_accuracy did not improve from 0.98390\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 0.0575 - val_accuracy: 0.9837\n",
      "Epoch 7/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0269 - accuracy: 0.9918\n",
      "Epoch 00007: val_accuracy improved from 0.98390 to 0.98740, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0269 - accuracy: 0.9918 - val_loss: 0.0445 - val_accuracy: 0.9874\n",
      "Epoch 8/100\n",
      "1856/1875 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9936\n",
      "Epoch 00008: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.0564 - val_accuracy: 0.9853\n",
      "Epoch 9/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9937\n",
      "Epoch 00009: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0207 - accuracy: 0.9937 - val_loss: 0.0597 - val_accuracy: 0.9849\n",
      "Epoch 10/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9947\n",
      "Epoch 00010: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0168 - accuracy: 0.9947 - val_loss: 0.0558 - val_accuracy: 0.9856\n",
      "Epoch 11/100\n",
      "1851/1875 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9948\n",
      "Epoch 00011: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0586 - val_accuracy: 0.9859\n",
      "Epoch 12/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 00012: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0143 - accuracy: 0.9956 - val_loss: 0.0649 - val_accuracy: 0.9851\n",
      "Epoch 13/100\n",
      "1853/1875 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9962\n",
      "Epoch 00013: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0129 - accuracy: 0.9962 - val_loss: 0.0720 - val_accuracy: 0.9840\n",
      "Epoch 14/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9963\n",
      "Epoch 00014: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.0700 - val_accuracy: 0.9851\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9965\n",
      "Epoch 00015: val_accuracy did not improve from 0.98740\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0117 - accuracy: 0.9965 - val_loss: 0.0634 - val_accuracy: 0.9863\n",
      "Epoch 16/100\n",
      "1858/1875 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9968\n",
      "Epoch 00016: val_accuracy improved from 0.98740 to 0.98770, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0107 - accuracy: 0.9968 - val_loss: 0.0621 - val_accuracy: 0.9877\n",
      "Epoch 17/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9965\n",
      "Epoch 00017: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 0.0704 - val_accuracy: 0.9864\n",
      "Epoch 18/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9973\n",
      "Epoch 00018: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0599 - val_accuracy: 0.9871\n",
      "Epoch 19/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9978\n",
      "Epoch 00019: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0701 - val_accuracy: 0.9857\n",
      "Epoch 20/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9970\n",
      "Epoch 00020: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0672 - val_accuracy: 0.9875\n",
      "Epoch 21/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9976\n",
      "Epoch 00021: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0656 - val_accuracy: 0.9862\n",
      "Epoch 22/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9974\n",
      "Epoch 00022: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0727 - val_accuracy: 0.9870\n",
      "Epoch 23/100\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9982\n",
      "Epoch 00023: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0778 - val_accuracy: 0.9875\n",
      "Epoch 24/100\n",
      "1847/1875 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9976\n",
      "Epoch 00024: val_accuracy did not improve from 0.98770\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.0866 - val_accuracy: 0.9856\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9978\n",
      "Epoch 00025: val_accuracy improved from 0.98770 to 0.98840, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0080 - accuracy: 0.9978 - val_loss: 0.0658 - val_accuracy: 0.9884\n",
      "Epoch 26/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9977\n",
      "Epoch 00026: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 0.0723 - val_accuracy: 0.9862\n",
      "Epoch 27/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9979\n",
      "Epoch 00027: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0923 - val_accuracy: 0.9842\n",
      "Epoch 28/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0062 - accuracy: 0.9982\n",
      "Epoch 00028: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0061 - accuracy: 0.9982 - val_loss: 0.0855 - val_accuracy: 0.9882\n",
      "Epoch 29/100\n",
      "1850/1875 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9979\n",
      "Epoch 00029: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0833 - val_accuracy: 0.9832\n",
      "Epoch 30/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9986\n",
      "Epoch 00030: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.0782 - val_accuracy: 0.9872\n",
      "Epoch 31/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 00031: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0928 - val_accuracy: 0.9864\n",
      "Epoch 32/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9983\n",
      "Epoch 00032: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.0904 - val_accuracy: 0.9865\n",
      "Epoch 33/100\n",
      "1855/1875 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9982\n",
      "Epoch 00033: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0789 - val_accuracy: 0.9861\n",
      "Epoch 34/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 00034: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0937 - val_accuracy: 0.9854\n",
      "Epoch 35/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0055 - accuracy: 0.9983\n",
      "Epoch 00035: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0970 - val_accuracy: 0.9844\n",
      "Epoch 36/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 00036: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0795 - val_accuracy: 0.9857\n",
      "Epoch 37/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 00037: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.1272 - val_accuracy: 0.9816\n",
      "Epoch 38/100\n",
      "1861/1875 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9979\n",
      "Epoch 00038: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.1118 - val_accuracy: 0.9842\n",
      "Epoch 39/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9983\n",
      "Epoch 00039: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0911 - val_accuracy: 0.9854\n",
      "Epoch 40/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 00040: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.1033 - val_accuracy: 0.9853\n",
      "Epoch 41/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 00041: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0993 - val_accuracy: 0.9857\n",
      "Epoch 42/100\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9986\n",
      "Epoch 00042: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1148 - val_accuracy: 0.9869\n",
      "Epoch 43/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 00043: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.1199 - val_accuracy: 0.9861\n",
      "Epoch 44/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9986\n",
      "Epoch 00044: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.1196 - val_accuracy: 0.9845\n",
      "Epoch 45/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 00045: val_accuracy did not improve from 0.98840\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.1206 - val_accuracy: 0.9858\n",
      "Epoch 00045: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('../models/48_1split_orig_class.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=32, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16-split classification without translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "def embed_and_translate(data, n_width, n_height):\n",
    "    ndata = np.zeros((len(data), n_width, n_height, 1), dtype='float32')\n",
    "    translations = np.empty((len(data), 2), dtype='float32')\n",
    "    width, height = data.shape[1], data.shape[2]\n",
    "    x, y = 0, 0\n",
    "    for i in range(len(data)):\n",
    "#         x = np.random.randint(n_width-width)\n",
    "#         y = np.random.randint(n_height-height)\n",
    "        ndata[i][x:x+width, y:y+height] = data[i] # rows, cols = height, width\n",
    "        translations[i][0] = x+(width//2)\n",
    "        translations[i][1] = y+(height//2)\n",
    "    return ndata, translations\n",
    "            \n",
    "n_splits = 16\n",
    "output_shape = (56, 56, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "latent_dim = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_augmented, y_train_regr = embed_and_translate(x_train, output_shape[0], output_shape[1])\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train_augmented], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_augmented, y_test_regr = embed_and_translate(x_test, output_shape[0], output_shape[1])\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test_augmented], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "        self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "        self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "        y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "        y_regr = self.regress(z ,training=training)\n",
    "        return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Activation('sigmoid')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'decoder_out' : 'mse',\n",
    "    'classifier_out' : 'categorical_crossentropy',\n",
    "    'regressor_out' : 'mean_absolute_percentage_error'\n",
    "}\n",
    "loss_weights = {\n",
    "    'decoder_out' : 0.0,\n",
    "    'classifier_out' : 1.0,\n",
    "    'regressor_out' : 0.0\n",
    "}\n",
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss=losses, loss_weights=loss_weights, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.8748 - classifier_out_loss: 0.8748 - decoder_out_loss: 0.2443 - regressor_out_loss: 101.0705 - classifier_out_accuracy: 0.7037 - decoder_out_accuracy: 0.5618 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00001: val_classifier_out_accuracy improved from -inf to 0.86850, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.8741 - classifier_out_loss: 0.8741 - decoder_out_loss: 0.2443 - regressor_out_loss: 101.0703 - classifier_out_accuracy: 0.7040 - decoder_out_accuracy: 0.5618 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.4045 - val_classifier_out_loss: 0.4045 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9339 - val_classifier_out_accuracy: 0.8685 - val_decoder_out_accuracy: 0.4868 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.3677 - classifier_out_loss: 0.3677 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9421 - classifier_out_accuracy: 0.8859 - decoder_out_accuracy: 0.5610 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00002: val_classifier_out_accuracy improved from 0.86850 to 0.91380, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.3675 - classifier_out_loss: 0.3675 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9421 - classifier_out_accuracy: 0.8860 - decoder_out_accuracy: 0.5611 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.2777 - val_classifier_out_loss: 0.2777 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9217 - val_classifier_out_accuracy: 0.9138 - val_decoder_out_accuracy: 0.4848 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.2721 - classifier_out_loss: 0.2721 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9390 - classifier_out_accuracy: 0.9150 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00003: val_classifier_out_accuracy improved from 0.91380 to 0.92390, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.2721 - classifier_out_loss: 0.2721 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9390 - classifier_out_accuracy: 0.9150 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.2491 - val_classifier_out_loss: 0.2491 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9257 - val_classifier_out_accuracy: 0.9239 - val_decoder_out_accuracy: 0.4843 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2326 - classifier_out_loss: 0.2326 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9385 - classifier_out_accuracy: 0.9272 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00004: val_classifier_out_accuracy improved from 0.92390 to 0.93410, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.2326 - classifier_out_loss: 0.2326 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9385 - classifier_out_accuracy: 0.9272 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.2199 - val_classifier_out_loss: 0.2199 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.8902 - val_classifier_out_accuracy: 0.9341 - val_decoder_out_accuracy: 0.4841 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.2074 - classifier_out_loss: 0.2074 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9304 - classifier_out_accuracy: 0.9357 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00005: val_classifier_out_accuracy improved from 0.93410 to 0.94540, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.2073 - classifier_out_loss: 0.2073 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9305 - classifier_out_accuracy: 0.9357 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1791 - val_classifier_out_loss: 0.1791 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9633 - val_classifier_out_accuracy: 0.9454 - val_decoder_out_accuracy: 0.4845 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1941 - classifier_out_loss: 0.1941 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9359 - classifier_out_accuracy: 0.9390 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00006: val_classifier_out_accuracy improved from 0.94540 to 0.94590, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.1941 - classifier_out_loss: 0.1941 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9359 - classifier_out_accuracy: 0.9391 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1706 - val_classifier_out_loss: 0.1706 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9694 - val_classifier_out_accuracy: 0.9459 - val_decoder_out_accuracy: 0.4848 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1753 - classifier_out_loss: 0.1753 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9365 - classifier_out_accuracy: 0.9457 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00007: val_classifier_out_accuracy did not improve from 0.94590\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1755 - classifier_out_loss: 0.1755 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9365 - classifier_out_accuracy: 0.9457 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1836 - val_classifier_out_loss: 0.1836 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9438 - val_classifier_out_accuracy: 0.9401 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1673 - classifier_out_loss: 0.1673 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9380 - classifier_out_accuracy: 0.9479 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00008: val_classifier_out_accuracy improved from 0.94590 to 0.94700, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1673 - classifier_out_loss: 0.1673 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9380 - classifier_out_accuracy: 0.9479 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1723 - val_classifier_out_loss: 0.1723 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9299 - val_classifier_out_accuracy: 0.9470 - val_decoder_out_accuracy: 0.4842 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1572 - classifier_out_loss: 0.1572 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9406 - classifier_out_accuracy: 0.9512 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00009: val_classifier_out_accuracy improved from 0.94700 to 0.95610, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1575 - classifier_out_loss: 0.1575 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9405 - classifier_out_accuracy: 0.9512 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1430 - val_classifier_out_loss: 0.1430 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9158 - val_classifier_out_accuracy: 0.9561 - val_decoder_out_accuracy: 0.4844 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1471 - classifier_out_loss: 0.1471 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9488 - classifier_out_accuracy: 0.9538 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00010: val_classifier_out_accuracy improved from 0.95610 to 0.95720, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1471 - classifier_out_loss: 0.1471 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9488 - classifier_out_accuracy: 0.9538 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1462 - val_classifier_out_loss: 0.1462 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9656 - val_classifier_out_accuracy: 0.9572 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1410 - classifier_out_loss: 0.1410 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9425 - classifier_out_accuracy: 0.9565 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00011: val_classifier_out_accuracy did not improve from 0.95720\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.1410 - classifier_out_loss: 0.1410 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9425 - classifier_out_accuracy: 0.9565 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.2214 - val_classifier_out_loss: 0.2214 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9230 - val_classifier_out_accuracy: 0.9332 - val_decoder_out_accuracy: 0.4845 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1353 - classifier_out_loss: 0.1353 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9490 - classifier_out_accuracy: 0.9578 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00012: val_classifier_out_accuracy improved from 0.95720 to 0.95920, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1353 - classifier_out_loss: 0.1353 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9489 - classifier_out_accuracy: 0.9578 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1273 - val_classifier_out_loss: 0.1273 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9297 - val_classifier_out_accuracy: 0.9592 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1316 - classifier_out_loss: 0.1316 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9430 - classifier_out_accuracy: 0.9587 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00013: val_classifier_out_accuracy did not improve from 0.95920\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1316 - classifier_out_loss: 0.1316 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9430 - classifier_out_accuracy: 0.9587 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1479 - val_classifier_out_loss: 0.1479 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9044 - val_classifier_out_accuracy: 0.9533 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1274 - classifier_out_loss: 0.1274 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9400 - classifier_out_accuracy: 0.9606 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00014: val_classifier_out_accuracy did not improve from 0.95920\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1275 - classifier_out_loss: 0.1275 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9400 - classifier_out_accuracy: 0.9605 - decoder_out_accuracy: 0.5609 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1467 - val_classifier_out_loss: 0.1467 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9437 - val_classifier_out_accuracy: 0.9557 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1206 - classifier_out_loss: 0.1206 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9339 - classifier_out_accuracy: 0.9615 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00015: val_classifier_out_accuracy did not improve from 0.95920\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1206 - classifier_out_loss: 0.1206 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9339 - classifier_out_accuracy: 0.9615 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1479 - val_classifier_out_loss: 0.1479 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9228 - val_classifier_out_accuracy: 0.9535 - val_decoder_out_accuracy: 0.4845 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1152 - classifier_out_loss: 0.1152 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9445 - classifier_out_accuracy: 0.9639 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00016: val_classifier_out_accuracy did not improve from 0.95920\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1151 - classifier_out_loss: 0.1151 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9445 - classifier_out_accuracy: 0.9639 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1409 - val_classifier_out_loss: 0.1409 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9450 - val_classifier_out_accuracy: 0.9591 - val_decoder_out_accuracy: 0.4843 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1102 - classifier_out_loss: 0.1102 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9414 - classifier_out_accuracy: 0.9652 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00017: val_classifier_out_accuracy did not improve from 0.95920\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1103 - classifier_out_loss: 0.1103 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9414 - classifier_out_accuracy: 0.9652 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1826 - val_classifier_out_loss: 0.1826 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9353 - val_classifier_out_accuracy: 0.9434 - val_decoder_out_accuracy: 0.4843 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.1084 - classifier_out_loss: 0.1084 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9458 - classifier_out_accuracy: 0.9661 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00018: val_classifier_out_accuracy did not improve from 0.95920\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1084 - classifier_out_loss: 0.1084 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9458 - classifier_out_accuracy: 0.9661 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1498 - val_classifier_out_loss: 0.1498 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9238 - val_classifier_out_accuracy: 0.9558 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1061 - classifier_out_loss: 0.1061 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9463 - classifier_out_accuracy: 0.9663 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00019: val_classifier_out_accuracy improved from 0.95920 to 0.96280, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.1063 - classifier_out_loss: 0.1063 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9463 - classifier_out_accuracy: 0.9663 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1197 - val_classifier_out_loss: 0.1197 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9677 - val_classifier_out_accuracy: 0.9628 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1045 - classifier_out_loss: 0.1045 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9325 - classifier_out_accuracy: 0.9665 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00020: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.1045 - classifier_out_loss: 0.1045 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9325 - classifier_out_accuracy: 0.9665 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1833 - val_classifier_out_loss: 0.1833 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9343 - val_classifier_out_accuracy: 0.9432 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0986 - classifier_out_loss: 0.0986 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9404 - classifier_out_accuracy: 0.9690 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00021: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0986 - classifier_out_loss: 0.0986 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9404 - classifier_out_accuracy: 0.9690 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1349 - val_classifier_out_loss: 0.1349 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9724 - val_classifier_out_accuracy: 0.9603 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1002 - classifier_out_loss: 0.1002 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9505 - classifier_out_accuracy: 0.9684 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00022: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1001 - classifier_out_loss: 0.1001 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9505 - classifier_out_accuracy: 0.9685 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1280 - val_classifier_out_loss: 0.1280 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9332 - val_classifier_out_accuracy: 0.9605 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0933 - classifier_out_loss: 0.0933 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9349 - classifier_out_accuracy: 0.9703 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00023: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0933 - classifier_out_loss: 0.0933 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9349 - classifier_out_accuracy: 0.9703 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1320 - val_classifier_out_loss: 0.1320 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9846 - val_classifier_out_accuracy: 0.9602 - val_decoder_out_accuracy: 0.4838 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0916 - classifier_out_loss: 0.0916 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9404 - classifier_out_accuracy: 0.9708 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00024: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0914 - classifier_out_loss: 0.0914 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9404 - classifier_out_accuracy: 0.9709 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1298 - val_classifier_out_loss: 0.1298 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9331 - val_classifier_out_accuracy: 0.9610 - val_decoder_out_accuracy: 0.4845 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0904 - classifier_out_loss: 0.0904 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9453 - classifier_out_accuracy: 0.9712 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00025: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0904 - classifier_out_loss: 0.0904 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9454 - classifier_out_accuracy: 0.9712 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1416 - val_classifier_out_loss: 0.1416 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9276 - val_classifier_out_accuracy: 0.9585 - val_decoder_out_accuracy: 0.4842 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0884 - classifier_out_loss: 0.0884 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9443 - classifier_out_accuracy: 0.9716 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00026: val_classifier_out_accuracy did not improve from 0.96280\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0885 - classifier_out_loss: 0.0885 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9444 - classifier_out_accuracy: 0.9716 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1323 - val_classifier_out_loss: 0.1323 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9883 - val_classifier_out_accuracy: 0.9593 - val_decoder_out_accuracy: 0.4857 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0837 - classifier_out_loss: 0.0837 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9459 - classifier_out_accuracy: 0.9735 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00027: val_classifier_out_accuracy improved from 0.96280 to 0.96430, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0837 - classifier_out_loss: 0.0837 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9459 - classifier_out_accuracy: 0.9735 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1256 - val_classifier_out_loss: 0.1256 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9711 - val_classifier_out_accuracy: 0.9643 - val_decoder_out_accuracy: 0.4853 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0816 - classifier_out_loss: 0.0816 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9383 - classifier_out_accuracy: 0.9738 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00028: val_classifier_out_accuracy improved from 0.96430 to 0.96630, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0817 - classifier_out_loss: 0.0817 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9383 - classifier_out_accuracy: 0.9738 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1150 - val_classifier_out_loss: 0.1150 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9256 - val_classifier_out_accuracy: 0.9663 - val_decoder_out_accuracy: 0.4850 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0808 - classifier_out_loss: 0.0808 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9377 - classifier_out_accuracy: 0.9743 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00029: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0808 - classifier_out_loss: 0.0808 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9377 - classifier_out_accuracy: 0.9743 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1310 - val_classifier_out_loss: 0.1310 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9145 - val_classifier_out_accuracy: 0.9616 - val_decoder_out_accuracy: 0.4855 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0779 - classifier_out_loss: 0.0779 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9350 - classifier_out_accuracy: 0.9749 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00030: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0779 - classifier_out_loss: 0.0779 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9350 - classifier_out_accuracy: 0.9749 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1300 - val_classifier_out_loss: 0.1300 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9303 - val_classifier_out_accuracy: 0.9625 - val_decoder_out_accuracy: 0.4850 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0796 - classifier_out_loss: 0.0796 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9484 - classifier_out_accuracy: 0.9745 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00031: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0796 - classifier_out_loss: 0.0796 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9485 - classifier_out_accuracy: 0.9745 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1156 - val_classifier_out_loss: 0.1156 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 101.0417 - val_classifier_out_accuracy: 0.9658 - val_decoder_out_accuracy: 0.4858 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0748 - classifier_out_loss: 0.0748 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9473 - classifier_out_accuracy: 0.9759 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00032: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0748 - classifier_out_loss: 0.0748 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9473 - classifier_out_accuracy: 0.9759 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1498 - val_classifier_out_loss: 0.1498 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9156 - val_classifier_out_accuracy: 0.9571 - val_decoder_out_accuracy: 0.4851 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0746 - classifier_out_loss: 0.0746 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9440 - classifier_out_accuracy: 0.9757 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00033: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0746 - classifier_out_loss: 0.0746 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9440 - classifier_out_accuracy: 0.9757 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1327 - val_classifier_out_loss: 0.1327 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9564 - val_classifier_out_accuracy: 0.9636 - val_decoder_out_accuracy: 0.4844 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0737 - classifier_out_loss: 0.0737 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9356 - classifier_out_accuracy: 0.9761 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00034: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0739 - classifier_out_loss: 0.0739 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9356 - classifier_out_accuracy: 0.9761 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1206 - val_classifier_out_loss: 0.1206 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9505 - val_classifier_out_accuracy: 0.9649 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0721 - classifier_out_loss: 0.0721 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9344 - classifier_out_accuracy: 0.9761 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00035: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0721 - classifier_out_loss: 0.0721 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9344 - classifier_out_accuracy: 0.9761 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1189 - val_classifier_out_loss: 0.1189 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9688 - val_classifier_out_accuracy: 0.9656 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0721 - classifier_out_loss: 0.0721 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9526 - classifier_out_accuracy: 0.9765 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00036: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0720 - classifier_out_loss: 0.0720 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9526 - classifier_out_accuracy: 0.9765 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1286 - val_classifier_out_loss: 0.1286 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9219 - val_classifier_out_accuracy: 0.9629 - val_decoder_out_accuracy: 0.4848 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0700 - classifier_out_loss: 0.0700 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9466 - classifier_out_accuracy: 0.9771 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00037: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0700 - classifier_out_loss: 0.0700 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9466 - classifier_out_accuracy: 0.9771 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1232 - val_classifier_out_loss: 0.1232 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9601 - val_classifier_out_accuracy: 0.9650 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0675 - classifier_out_loss: 0.0675 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9593 - classifier_out_accuracy: 0.9785 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00038: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0675 - classifier_out_loss: 0.0675 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9593 - classifier_out_accuracy: 0.9785 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1254 - val_classifier_out_loss: 0.1254 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9180 - val_classifier_out_accuracy: 0.9650 - val_decoder_out_accuracy: 0.4844 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0665 - classifier_out_loss: 0.0665 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9448 - classifier_out_accuracy: 0.9785 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00039: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0664 - classifier_out_loss: 0.0664 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9448 - classifier_out_accuracy: 0.9785 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1722 - val_classifier_out_loss: 0.1722 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9324 - val_classifier_out_accuracy: 0.9525 - val_decoder_out_accuracy: 0.4853 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0636 - classifier_out_loss: 0.0636 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9494 - classifier_out_accuracy: 0.9787 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00040: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0636 - classifier_out_loss: 0.0636 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9494 - classifier_out_accuracy: 0.9787 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1341 - val_classifier_out_loss: 0.1341 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9385 - val_classifier_out_accuracy: 0.9621 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0654 - classifier_out_loss: 0.0654 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9448 - classifier_out_accuracy: 0.9784 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00041: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0653 - classifier_out_loss: 0.0653 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9448 - classifier_out_accuracy: 0.9784 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1342 - val_classifier_out_loss: 0.1342 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9374 - val_classifier_out_accuracy: 0.9611 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0635 - classifier_out_loss: 0.0635 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9553 - classifier_out_accuracy: 0.9799 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00042: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0636 - classifier_out_loss: 0.0636 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9553 - classifier_out_accuracy: 0.9798 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1243 - val_classifier_out_loss: 0.1243 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9784 - val_classifier_out_accuracy: 0.9642 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0594 - classifier_out_loss: 0.0594 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9481 - classifier_out_accuracy: 0.9807 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00043: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0594 - classifier_out_loss: 0.0594 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9481 - classifier_out_accuracy: 0.9807 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1627 - val_classifier_out_loss: 0.1627 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9607 - val_classifier_out_accuracy: 0.9542 - val_decoder_out_accuracy: 0.4853 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0588 - classifier_out_loss: 0.0588 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9467 - classifier_out_accuracy: 0.9808 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00044: val_classifier_out_accuracy did not improve from 0.96630\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0588 - classifier_out_loss: 0.0588 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9467 - classifier_out_accuracy: 0.9808 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1407 - val_classifier_out_loss: 0.1407 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9437 - val_classifier_out_accuracy: 0.9630 - val_decoder_out_accuracy: 0.4850 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0605 - classifier_out_loss: 0.0605 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9495 - classifier_out_accuracy: 0.9797 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00045: val_classifier_out_accuracy improved from 0.96630 to 0.96640, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0605 - classifier_out_loss: 0.0605 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9495 - classifier_out_accuracy: 0.9797 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1197 - val_classifier_out_loss: 0.1197 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9373 - val_classifier_out_accuracy: 0.9664 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0577 - classifier_out_loss: 0.0577 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9445 - classifier_out_accuracy: 0.9814 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00046: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0578 - classifier_out_loss: 0.0578 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9445 - classifier_out_accuracy: 0.9813 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1489 - val_classifier_out_loss: 0.1489 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9140 - val_classifier_out_accuracy: 0.9593 - val_decoder_out_accuracy: 0.4856 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0557 - classifier_out_loss: 0.0557 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9433 - classifier_out_accuracy: 0.9815 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00047: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0557 - classifier_out_loss: 0.0557 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9433 - classifier_out_accuracy: 0.9815 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1407 - val_classifier_out_loss: 0.1407 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9197 - val_classifier_out_accuracy: 0.9638 - val_decoder_out_accuracy: 0.4858 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0583 - classifier_out_loss: 0.0583 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9398 - classifier_out_accuracy: 0.9811 - decoder_out_accuracy: 0.5605 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00048: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0582 - classifier_out_loss: 0.0582 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9398 - classifier_out_accuracy: 0.9811 - decoder_out_accuracy: 0.5605 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1288 - val_classifier_out_loss: 0.1288 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9489 - val_classifier_out_accuracy: 0.9664 - val_decoder_out_accuracy: 0.4857 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0531 - classifier_out_loss: 0.0531 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9395 - classifier_out_accuracy: 0.9826 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00049: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0531 - classifier_out_loss: 0.0531 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9395 - classifier_out_accuracy: 0.9826 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1365 - val_classifier_out_loss: 0.1365 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9588 - val_classifier_out_accuracy: 0.9644 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0558 - classifier_out_loss: 0.0558 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9394 - classifier_out_accuracy: 0.9819 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00050: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0559 - classifier_out_loss: 0.0559 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9395 - classifier_out_accuracy: 0.9819 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1794 - val_classifier_out_loss: 0.1794 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9803 - val_classifier_out_accuracy: 0.9516 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0525 - classifier_out_loss: 0.0525 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9506 - classifier_out_accuracy: 0.9828 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00051: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0525 - classifier_out_loss: 0.0525 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9506 - classifier_out_accuracy: 0.9828 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1337 - val_classifier_out_loss: 0.1337 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.8849 - val_classifier_out_accuracy: 0.9648 - val_decoder_out_accuracy: 0.4852 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0538 - classifier_out_loss: 0.0538 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9492 - classifier_out_accuracy: 0.9827 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00052: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0538 - classifier_out_loss: 0.0538 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9492 - classifier_out_accuracy: 0.9827 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1410 - val_classifier_out_loss: 0.1410 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9627 - val_classifier_out_accuracy: 0.9631 - val_decoder_out_accuracy: 0.4848 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0533 - classifier_out_loss: 0.0533 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9376 - classifier_out_accuracy: 0.9823 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00053: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0532 - classifier_out_loss: 0.0532 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9376 - classifier_out_accuracy: 0.9823 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1342 - val_classifier_out_loss: 0.1342 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9281 - val_classifier_out_accuracy: 0.9649 - val_decoder_out_accuracy: 0.4855 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0526 - classifier_out_loss: 0.0526 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9416 - classifier_out_accuracy: 0.9826 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00054: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0526 - classifier_out_loss: 0.0526 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9416 - classifier_out_accuracy: 0.9826 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1603 - val_classifier_out_loss: 0.1603 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9709 - val_classifier_out_accuracy: 0.9568 - val_decoder_out_accuracy: 0.4859 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0500 - classifier_out_loss: 0.0500 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9456 - classifier_out_accuracy: 0.9835 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00055: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0501 - classifier_out_loss: 0.0501 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9456 - classifier_out_accuracy: 0.9835 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1704 - val_classifier_out_loss: 0.1704 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9843 - val_classifier_out_accuracy: 0.9570 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0504 - classifier_out_loss: 0.0504 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9434 - classifier_out_accuracy: 0.9836 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00056: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0505 - classifier_out_loss: 0.0505 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9434 - classifier_out_accuracy: 0.9835 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1404 - val_classifier_out_loss: 0.1404 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9430 - val_classifier_out_accuracy: 0.9638 - val_decoder_out_accuracy: 0.4845 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0508 - classifier_out_loss: 0.0508 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9245 - classifier_out_accuracy: 0.9831 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00057: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0507 - classifier_out_loss: 0.0507 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9245 - classifier_out_accuracy: 0.9831 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1383 - val_classifier_out_loss: 0.1383 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9190 - val_classifier_out_accuracy: 0.9641 - val_decoder_out_accuracy: 0.4856 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0475 - classifier_out_loss: 0.0475 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9459 - classifier_out_accuracy: 0.9843 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00058: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0475 - classifier_out_loss: 0.0475 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9459 - classifier_out_accuracy: 0.9843 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1416 - val_classifier_out_loss: 0.1416 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9737 - val_classifier_out_accuracy: 0.9629 - val_decoder_out_accuracy: 0.4856 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0477 - classifier_out_loss: 0.0477 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9443 - classifier_out_accuracy: 0.9843 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00059: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0476 - classifier_out_loss: 0.0476 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9443 - classifier_out_accuracy: 0.9844 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1363 - val_classifier_out_loss: 0.1363 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9199 - val_classifier_out_accuracy: 0.9645 - val_decoder_out_accuracy: 0.4853 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0484 - classifier_out_loss: 0.0484 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9386 - classifier_out_accuracy: 0.9839 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00060: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0484 - classifier_out_loss: 0.0484 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9386 - classifier_out_accuracy: 0.9839 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1572 - val_classifier_out_loss: 0.1572 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9544 - val_classifier_out_accuracy: 0.9591 - val_decoder_out_accuracy: 0.4856 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0436 - classifier_out_loss: 0.0436 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9399 - classifier_out_accuracy: 0.9854 - decoder_out_accuracy: 0.5605 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00061: val_classifier_out_accuracy did not improve from 0.96640\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.0437 - classifier_out_loss: 0.0437 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9399 - classifier_out_accuracy: 0.9853 - decoder_out_accuracy: 0.5605 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1878 - val_classifier_out_loss: 0.1878 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9380 - val_classifier_out_accuracy: 0.9595 - val_decoder_out_accuracy: 0.4851 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0500 - classifier_out_loss: 0.0500 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9415 - classifier_out_accuracy: 0.9831 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00062: val_classifier_out_accuracy improved from 0.96640 to 0.96760, saving model to ../models/48_1split_orig_class.h5\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0500 - classifier_out_loss: 0.0500 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9415 - classifier_out_accuracy: 0.9831 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1327 - val_classifier_out_loss: 0.1327 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9411 - val_classifier_out_accuracy: 0.9676 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0467 - classifier_out_loss: 0.0467 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9489 - classifier_out_accuracy: 0.9842 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00063: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0466 - classifier_out_loss: 0.0466 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9489 - classifier_out_accuracy: 0.9842 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1402 - val_classifier_out_loss: 0.1402 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9463 - val_classifier_out_accuracy: 0.9654 - val_decoder_out_accuracy: 0.4854 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0439 - classifier_out_loss: 0.0439 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9454 - classifier_out_accuracy: 0.9855 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00064: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0440 - classifier_out_loss: 0.0440 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9454 - classifier_out_accuracy: 0.9855 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1606 - val_classifier_out_loss: 0.1606 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9536 - val_classifier_out_accuracy: 0.9625 - val_decoder_out_accuracy: 0.4853 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0443 - classifier_out_loss: 0.0443 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9311 - classifier_out_accuracy: 0.9850 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00065: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0443 - classifier_out_loss: 0.0443 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9311 - classifier_out_accuracy: 0.9850 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1525 - val_classifier_out_loss: 0.1525 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9432 - val_classifier_out_accuracy: 0.9630 - val_decoder_out_accuracy: 0.4865 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0434 - classifier_out_loss: 0.0434 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9344 - classifier_out_accuracy: 0.9855 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00066: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0434 - classifier_out_loss: 0.0434 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9345 - classifier_out_accuracy: 0.9855 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1626 - val_classifier_out_loss: 0.1626 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9766 - val_classifier_out_accuracy: 0.9608 - val_decoder_out_accuracy: 0.4850 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0445 - classifier_out_loss: 0.0445 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9321 - classifier_out_accuracy: 0.9855 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00067: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0445 - classifier_out_loss: 0.0445 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9321 - classifier_out_accuracy: 0.9855 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1400 - val_classifier_out_loss: 0.1400 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9509 - val_classifier_out_accuracy: 0.9639 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0443 - classifier_out_loss: 0.0443 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9468 - classifier_out_accuracy: 0.9854 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00068: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0443 - classifier_out_loss: 0.0443 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9468 - classifier_out_accuracy: 0.9854 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1595 - val_classifier_out_loss: 0.1595 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9764 - val_classifier_out_accuracy: 0.9603 - val_decoder_out_accuracy: 0.4850 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0440 - classifier_out_loss: 0.0440 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9623 - classifier_out_accuracy: 0.9849 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00069: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0440 - classifier_out_loss: 0.0440 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9623 - classifier_out_accuracy: 0.9850 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1523 - val_classifier_out_loss: 0.1523 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9498 - val_classifier_out_accuracy: 0.9624 - val_decoder_out_accuracy: 0.4860 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0438 - classifier_out_loss: 0.0438 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9482 - classifier_out_accuracy: 0.9853 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00070: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0438 - classifier_out_loss: 0.0438 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9482 - classifier_out_accuracy: 0.9853 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1417 - val_classifier_out_loss: 0.1417 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9742 - val_classifier_out_accuracy: 0.9631 - val_decoder_out_accuracy: 0.4850 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0389 - classifier_out_loss: 0.0389 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9465 - classifier_out_accuracy: 0.9869 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00071: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0389 - classifier_out_loss: 0.0389 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9465 - classifier_out_accuracy: 0.9869 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1691 - val_classifier_out_loss: 0.1691 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9695 - val_classifier_out_accuracy: 0.9602 - val_decoder_out_accuracy: 0.4854 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0419 - classifier_out_loss: 0.0419 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9463 - classifier_out_accuracy: 0.9862 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00072: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0419 - classifier_out_loss: 0.0419 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9463 - classifier_out_accuracy: 0.9862 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1600 - val_classifier_out_loss: 0.1600 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9402 - val_classifier_out_accuracy: 0.9595 - val_decoder_out_accuracy: 0.4854 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0389 - classifier_out_loss: 0.0389 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9396 - classifier_out_accuracy: 0.9870 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00073: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0389 - classifier_out_loss: 0.0389 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9396 - classifier_out_accuracy: 0.9870 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1554 - val_classifier_out_loss: 0.1554 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9782 - val_classifier_out_accuracy: 0.9613 - val_decoder_out_accuracy: 0.4860 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0425 - classifier_out_loss: 0.0425 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9569 - classifier_out_accuracy: 0.9857 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00074: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0426 - classifier_out_loss: 0.0426 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9569 - classifier_out_accuracy: 0.9857 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1599 - val_classifier_out_loss: 0.1599 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9737 - val_classifier_out_accuracy: 0.9626 - val_decoder_out_accuracy: 0.4847 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0421 - classifier_out_loss: 0.0421 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9492 - classifier_out_accuracy: 0.9857 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00075: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0421 - classifier_out_loss: 0.0421 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9492 - classifier_out_accuracy: 0.9857 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1635 - val_classifier_out_loss: 0.1635 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9171 - val_classifier_out_accuracy: 0.9621 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0384 - classifier_out_loss: 0.0384 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9405 - classifier_out_accuracy: 0.9866 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00076: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0384 - classifier_out_loss: 0.0384 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9405 - classifier_out_accuracy: 0.9866 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1678 - val_classifier_out_loss: 0.1678 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9738 - val_classifier_out_accuracy: 0.9616 - val_decoder_out_accuracy: 0.4846 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0389 - classifier_out_loss: 0.0389 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9460 - classifier_out_accuracy: 0.9871 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00077: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.0389 - classifier_out_loss: 0.0389 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9460 - classifier_out_accuracy: 0.9871 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1822 - val_classifier_out_loss: 0.1822 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9665 - val_classifier_out_accuracy: 0.9596 - val_decoder_out_accuracy: 0.4849 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0397 - classifier_out_loss: 0.0397 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9451 - classifier_out_accuracy: 0.9865 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00078: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0397 - classifier_out_loss: 0.0397 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9451 - classifier_out_accuracy: 0.9865 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1658 - val_classifier_out_loss: 0.1658 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9205 - val_classifier_out_accuracy: 0.9630 - val_decoder_out_accuracy: 0.4853 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0353 - classifier_out_loss: 0.0353 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9439 - classifier_out_accuracy: 0.9879 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00079: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0352 - classifier_out_loss: 0.0352 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9439 - classifier_out_accuracy: 0.9879 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1586 - val_classifier_out_loss: 0.1586 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9164 - val_classifier_out_accuracy: 0.9622 - val_decoder_out_accuracy: 0.4851 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0392 - classifier_out_loss: 0.0392 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9577 - classifier_out_accuracy: 0.9867 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00080: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0392 - classifier_out_loss: 0.0392 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9577 - classifier_out_accuracy: 0.9867 - decoder_out_accuracy: 0.5608 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1546 - val_classifier_out_loss: 0.1546 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9649 - val_classifier_out_accuracy: 0.9658 - val_decoder_out_accuracy: 0.4852 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0374 - classifier_out_loss: 0.0374 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9546 - classifier_out_accuracy: 0.9874 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00081: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0374 - classifier_out_loss: 0.0374 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9546 - classifier_out_accuracy: 0.9874 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.1591 - val_classifier_out_loss: 0.1591 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9852 - val_classifier_out_accuracy: 0.9589 - val_decoder_out_accuracy: 0.4852 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0361 - classifier_out_loss: 0.0361 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9495 - classifier_out_accuracy: 0.9878 - decoder_out_accuracy: 0.5606 - regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00082: val_classifier_out_accuracy did not improve from 0.96760\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0361 - classifier_out_loss: 0.0361 - decoder_out_loss: 0.2443 - regressor_out_loss: 100.9495 - classifier_out_accuracy: 0.9878 - decoder_out_accuracy: 0.5607 - regressor_out_accuracy: 0.0000e+00 - val_loss: 0.2039 - val_classifier_out_loss: 0.2039 - val_decoder_out_loss: 0.2306 - val_regressor_out_loss: 100.9437 - val_classifier_out_accuracy: 0.9561 - val_decoder_out_accuracy: 0.4851 - val_regressor_out_accuracy: 0.0000e+00\n",
      "Epoch 00082: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_classifier_out_accuracy', mode='max', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('../models/48_1split_orig_class.h5', monitor='val_classifier_out_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n",
    "                    validation_data=(x_test_split, {'decoder_out': x_test, 'classifier_out': y_test, 'regressor_out':y_test_regr}),\n",
    "                    epochs=100, batch_size=32, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 16-split with image translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "def embed_and_translate(data, n_width, n_height):\n",
    "    ndata = np.zeros((len(data), n_width, n_height, 1), dtype='float32')\n",
    "    translations = np.empty((len(data), 2), dtype='float32')\n",
    "    width, height = data.shape[1], data.shape[2]\n",
    "#     x, y = 0, 0\n",
    "    for i in range(len(data)):\n",
    "        x = np.random.randint(n_width-width)\n",
    "        y = np.random.randint(n_height-height)\n",
    "        ndata[i][x:x+width, y:y+height] = data[i] # rows, cols = height, width\n",
    "        translations[i][0] = x+(width//2)\n",
    "        translations[i][1] = y+(height//2)\n",
    "    return ndata, translations\n",
    "            \n",
    "n_splits = 16\n",
    "output_shape = (56, 56, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "latent_dim = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_augmented, y_train_regr = embed_and_translate(x_train, output_shape[0], output_shape[1])\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train_augmented], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_augmented, y_test_regr = embed_and_translate(x_test, output_shape[0], output_shape[1])\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test_augmented], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras import initializers\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "#         self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "#         self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "#         y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "#         y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "#         y_regr = self.regress(z ,training=training)\n",
    "#         return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        return y_class\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "#             Activation('sigmoid')\n",
    "            Activation('relu')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'decoder_out' : 'mse',\n",
    "    'classifier_out' : 'categorical_crossentropy',\n",
    "    'regressor_out' : 'mean_absolute_percentage_error'\n",
    "}\n",
    "loss_weights = {\n",
    "    'decoder_out' : 0.0,\n",
    "    'classifier_out' : 1.0,\n",
    "    'regressor_out' : 0.0\n",
    "}\n",
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 1.2475 - accuracy: 0.5443\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.69330, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 1.2466 - accuracy: 0.5448 - val_loss: 0.8858 - val_accuracy: 0.6933\n",
      "Epoch 2/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.7469 - accuracy: 0.7437\n",
      "Epoch 00002: val_accuracy improved from 0.69330 to 0.77150, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.7464 - accuracy: 0.7438 - val_loss: 0.6595 - val_accuracy: 0.7715\n",
      "Epoch 3/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.6069 - accuracy: 0.7916\n",
      "Epoch 00003: val_accuracy improved from 0.77150 to 0.80470, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.6064 - accuracy: 0.7918 - val_loss: 0.5671 - val_accuracy: 0.8047\n",
      "Epoch 4/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.5286 - accuracy: 0.8191\n",
      "Epoch 00004: val_accuracy improved from 0.80470 to 0.82690, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.5285 - accuracy: 0.8192 - val_loss: 0.5181 - val_accuracy: 0.8269\n",
      "Epoch 5/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.4765 - accuracy: 0.8382\n",
      "Epoch 00005: val_accuracy improved from 0.82690 to 0.82830, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4763 - accuracy: 0.8384 - val_loss: 0.5287 - val_accuracy: 0.8283\n",
      "Epoch 6/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.8530\n",
      "Epoch 00006: val_accuracy improved from 0.82830 to 0.83950, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4322 - accuracy: 0.8532 - val_loss: 0.4898 - val_accuracy: 0.8395\n",
      "Epoch 7/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.3974 - accuracy: 0.8648\n",
      "Epoch 00007: val_accuracy improved from 0.83950 to 0.85270, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3974 - accuracy: 0.8648 - val_loss: 0.4595 - val_accuracy: 0.8527\n",
      "Epoch 8/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8753\n",
      "Epoch 00008: val_accuracy did not improve from 0.85270\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3675 - accuracy: 0.8754 - val_loss: 0.4898 - val_accuracy: 0.8349\n",
      "Epoch 9/100\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.8839\n",
      "Epoch 00009: val_accuracy improved from 0.85270 to 0.85560, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3431 - accuracy: 0.8841 - val_loss: 0.4421 - val_accuracy: 0.8556\n",
      "Epoch 10/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.3213 - accuracy: 0.8912\n",
      "Epoch 00010: val_accuracy improved from 0.85560 to 0.86630, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3212 - accuracy: 0.8913 - val_loss: 0.4192 - val_accuracy: 0.8663\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3049 - accuracy: 0.8961\n",
      "Epoch 00011: val_accuracy improved from 0.86630 to 0.87570, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3049 - accuracy: 0.8961 - val_loss: 0.3902 - val_accuracy: 0.8757\n",
      "Epoch 12/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.9038\n",
      "Epoch 00012: val_accuracy did not improve from 0.87570\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2833 - accuracy: 0.9038 - val_loss: 0.4300 - val_accuracy: 0.8639\n",
      "Epoch 13/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.2694 - accuracy: 0.9079\n",
      "Epoch 00013: val_accuracy did not improve from 0.87570\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2695 - accuracy: 0.9079 - val_loss: 0.4182 - val_accuracy: 0.8690\n",
      "Epoch 14/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.9121\n",
      "Epoch 00014: val_accuracy did not improve from 0.87570\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2543 - accuracy: 0.9121 - val_loss: 0.3932 - val_accuracy: 0.8746\n",
      "Epoch 15/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9159\n",
      "Epoch 00015: val_accuracy improved from 0.87570 to 0.87890, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2439 - accuracy: 0.9158 - val_loss: 0.3974 - val_accuracy: 0.8789\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.9198\n",
      "Epoch 00016: val_accuracy did not improve from 0.87890\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2294 - accuracy: 0.9198 - val_loss: 0.4085 - val_accuracy: 0.8780\n",
      "Epoch 17/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.2186 - accuracy: 0.9240\n",
      "Epoch 00017: val_accuracy did not improve from 0.87890\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2186 - accuracy: 0.9241 - val_loss: 0.4175 - val_accuracy: 0.8770\n",
      "Epoch 18/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9270\n",
      "Epoch 00018: val_accuracy improved from 0.87890 to 0.88210, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2074 - accuracy: 0.9269 - val_loss: 0.3951 - val_accuracy: 0.8821\n",
      "Epoch 19/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9302\n",
      "Epoch 00019: val_accuracy improved from 0.88210 to 0.88370, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2004 - accuracy: 0.9301 - val_loss: 0.3976 - val_accuracy: 0.8837\n",
      "Epoch 20/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9333\n",
      "Epoch 00020: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1902 - accuracy: 0.9333 - val_loss: 0.4180 - val_accuracy: 0.8801\n",
      "Epoch 21/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.1810 - accuracy: 0.9375\n",
      "Epoch 00021: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1810 - accuracy: 0.9375 - val_loss: 0.4218 - val_accuracy: 0.8797\n",
      "Epoch 22/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.1725 - accuracy: 0.9387\n",
      "Epoch 00022: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1729 - accuracy: 0.9387 - val_loss: 0.4520 - val_accuracy: 0.8781\n",
      "Epoch 23/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9406\n",
      "Epoch 00023: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1685 - accuracy: 0.9406 - val_loss: 0.4463 - val_accuracy: 0.8751\n",
      "Epoch 24/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9432\n",
      "Epoch 00024: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1606 - accuracy: 0.9432 - val_loss: 0.4434 - val_accuracy: 0.8788\n",
      "Epoch 25/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9462\n",
      "Epoch 00025: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1532 - accuracy: 0.9462 - val_loss: 0.4470 - val_accuracy: 0.8791\n",
      "Epoch 26/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.1504 - accuracy: 0.9469\n",
      "Epoch 00026: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1504 - accuracy: 0.9469 - val_loss: 0.4730 - val_accuracy: 0.8800\n",
      "Epoch 27/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9496\n",
      "Epoch 00027: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1425 - accuracy: 0.9495 - val_loss: 0.4718 - val_accuracy: 0.8786\n",
      "Epoch 28/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.1388 - accuracy: 0.9498\n",
      "Epoch 00028: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1389 - accuracy: 0.9498 - val_loss: 0.4839 - val_accuracy: 0.8755\n",
      "Epoch 29/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.1335 - accuracy: 0.9527\n",
      "Epoch 00029: val_accuracy did not improve from 0.88370\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1335 - accuracy: 0.9528 - val_loss: 0.4732 - val_accuracy: 0.8801\n",
      "Epoch 30/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9541\n",
      "Epoch 00030: val_accuracy improved from 0.88370 to 0.88440, saving model to ../models/48_16split_trans_class.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1284 - accuracy: 0.9541 - val_loss: 0.4755 - val_accuracy: 0.8844\n",
      "Epoch 31/100\n",
      "1468/1875 [======================>.......] - ETA: 1s - loss: 0.1201 - accuracy: 0.9570"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1950d7201ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/48_16split_trans_class.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \"\"\"\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 617\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    517\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \"\"\"\n\u001b[1;32m    960\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('../models/48_16split_trans_class.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train_split, y_train, validation_data=(x_test_split, y_test), epochs=100, batch_size=32, callbacks=[es, mc])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
