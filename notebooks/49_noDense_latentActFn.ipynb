{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "enc_input (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "pt_conv1 (Conv2D)            (None, 4, 4, 1)           577       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "latent (Activation)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "Decoder (Model)              (None, 28, 28, 1)         30089     \n",
      "=================================================================\n",
      "Total params: 49,482\n",
      "Trainable params: 49,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# encoder\n",
    "enc_input = Input(shape=(28,28,1), name='enc_input')\n",
    "x  = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv1')(enc_input)\n",
    "x  = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='conv2')(x)\n",
    "x  = Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', name='pt_conv1')(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "latent = Activation('relu', name='latent')(x)\n",
    "\n",
    "# decoder\n",
    "dec_input = Input(shape=(16,), name='dec_input')\n",
    "x = Dense(units=7*7*8, activation='relu', name='dense')(dec_input)\n",
    "x = Reshape(target_shape=(7,7,8), name='reshape')(x)\n",
    "x = Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv1')(x)\n",
    "x = Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same', name='deconv2')(x)\n",
    "dec_output = Conv2DTranspose(filters=1, kernel_size=3, padding='same', name='pt_conv')(x)\n",
    "\n",
    "encoder = Model(enc_input, latent, name=\"Encoder\")\n",
    "decoder = Model(dec_input, dec_output, name=\"Decoder\")\n",
    "model = Model(encoder.input, decoder(encoder.output))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0295\n",
      "Epoch 00001: val_loss improved from inf to 0.02332, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0295 - val_loss: 0.0233\n",
      "Epoch 2/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0213\n",
      "Epoch 00002: val_loss improved from 0.02332 to 0.01998, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0213 - val_loss: 0.0200\n",
      "Epoch 3/100\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.0197\n",
      "Epoch 00003: val_loss improved from 0.01998 to 0.01907, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0197 - val_loss: 0.0191\n",
      "Epoch 4/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0189\n",
      "Epoch 00004: val_loss improved from 0.01907 to 0.01819, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0189 - val_loss: 0.0182\n",
      "Epoch 5/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0183\n",
      "Epoch 00005: val_loss improved from 0.01819 to 0.01756, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0183 - val_loss: 0.0176\n",
      "Epoch 6/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0178\n",
      "Epoch 00006: val_loss improved from 0.01756 to 0.01734, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0178 - val_loss: 0.0173\n",
      "Epoch 7/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0175\n",
      "Epoch 00007: val_loss improved from 0.01734 to 0.01723, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 8/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00008: val_loss improved from 0.01723 to 0.01687, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0172 - val_loss: 0.0169\n",
      "Epoch 9/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0170\n",
      "Epoch 00009: val_loss improved from 0.01687 to 0.01656, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0170 - val_loss: 0.0166\n",
      "Epoch 10/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00010: val_loss improved from 0.01656 to 0.01651, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0168 - val_loss: 0.0165\n",
      "Epoch 11/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00011: val_loss did not improve from 0.01651\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 12/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00012: val_loss improved from 0.01651 to 0.01618, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0165 - val_loss: 0.0162\n",
      "Epoch 13/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0163\n",
      "Epoch 00013: val_loss did not improve from 0.01618\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0163 - val_loss: 0.0163\n",
      "Epoch 14/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0162\n",
      "Epoch 00014: val_loss improved from 0.01618 to 0.01601, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0162 - val_loss: 0.0160\n",
      "Epoch 15/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0161\n",
      "Epoch 00015: val_loss did not improve from 0.01601\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 16/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00016: val_loss improved from 0.01601 to 0.01570, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0160 - val_loss: 0.0157\n",
      "Epoch 17/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0159\n",
      "Epoch 00017: val_loss did not improve from 0.01570\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0159 - val_loss: 0.0157\n",
      "Epoch 18/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0158\n",
      "Epoch 00018: val_loss improved from 0.01570 to 0.01567, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 19/100\n",
      "1862/1875 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00019: val_loss improved from 0.01567 to 0.01544, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0157 - val_loss: 0.0154\n",
      "Epoch 20/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0157\n",
      "Epoch 00020: val_loss improved from 0.01544 to 0.01537, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0157 - val_loss: 0.0154\n",
      "Epoch 21/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0156\n",
      "Epoch 00021: val_loss did not improve from 0.01537\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0156 - val_loss: 0.0154\n",
      "Epoch 22/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00022: val_loss did not improve from 0.01537\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 23/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00023: val_loss did not improve from 0.01537\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 24/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0155\n",
      "Epoch 00024: val_loss improved from 0.01537 to 0.01524, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0155 - val_loss: 0.0152\n",
      "Epoch 25/100\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.0154\n",
      "Epoch 00025: val_loss improved from 0.01524 to 0.01524, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0154 - val_loss: 0.0152\n",
      "Epoch 26/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0153\n",
      "Epoch 00026: val_loss did not improve from 0.01524\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0153 - val_loss: 0.0153\n",
      "Epoch 27/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0153\n",
      "Epoch 00027: val_loss improved from 0.01524 to 0.01515, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 28/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00028: val_loss did not improve from 0.01515\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 29/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00029: val_loss did not improve from 0.01515\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 30/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0152\n",
      "Epoch 00030: val_loss improved from 0.01515 to 0.01502, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0152 - val_loss: 0.0150\n",
      "Epoch 31/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00031: val_loss improved from 0.01502 to 0.01499, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 32/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00032: val_loss did not improve from 0.01499\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0151 - val_loss: 0.0151\n",
      "Epoch 33/100\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 0.0151\n",
      "Epoch 00033: val_loss did not improve from 0.01499\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 34/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00034: val_loss did not improve from 0.01499\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0150 - val_loss: 0.0151\n",
      "Epoch 35/100\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00035: val_loss did not improve from 0.01499\n",
      "1875/1875 [==============================] - 8s 5ms/step - loss: 0.0150 - val_loss: 0.0150\n",
      "Epoch 36/100\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00036: val_loss improved from 0.01499 to 0.01496, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0150 - val_loss: 0.0150\n",
      "Epoch 37/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0150\n",
      "Epoch 00037: val_loss improved from 0.01496 to 0.01494, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0150 - val_loss: 0.0149\n",
      "Epoch 38/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00038: val_loss improved from 0.01494 to 0.01491, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 39/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00039: val_loss did not improve from 0.01491\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0149 - val_loss: 0.0150\n",
      "Epoch 40/100\n",
      "1863/1875 [============================>.] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00040: val_loss improved from 0.01491 to 0.01486, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 41/100\n",
      "1868/1875 [============================>.] - ETA: 0s - loss: 0.0149\n",
      "Epoch 00041: val_loss did not improve from 0.01486\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0149 - val_loss: 0.0154\n",
      "Epoch 42/100\n",
      "1873/1875 [============================>.] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00042: val_loss improved from 0.01486 to 0.01471, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 43/100\n",
      "1867/1875 [============================>.] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00043: val_loss did not improve from 0.01471\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00044: val_loss did not improve from 0.01471\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0148 - val_loss: 0.0148\n",
      "Epoch 45/100\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.0148\n",
      "Epoch 00045: val_loss did not improve from 0.01471\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0148 - val_loss: 0.0150\n",
      "Epoch 46/100\n",
      "1869/1875 [============================>.] - ETA: 0s - loss: 0.0147\n",
      "Epoch 00046: val_loss did not improve from 0.01471\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0147 - val_loss: 0.0149\n",
      "Epoch 47/100\n",
      "1864/1875 [============================>.] - ETA: 0s - loss: 0.0147\n",
      "Epoch 00047: val_loss improved from 0.01471 to 0.01469, saving model to ../models/49_1split.h5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0147 - val_loss: 0.0147\n",
      "Epoch 48/100\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.0147"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c93ce4f065cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/49_1split.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2740\u001b[0;31m       \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2741\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m       raise TypeError(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__get_cmp_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;34m\"\"\"Returns a hashable eq-comparable key for `self`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;31m# TODO(b/133606651): Decide whether to cache this value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__make_cmp_key\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    325\u001b[0m       ])\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m       ])\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__make_cmp_key\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    325\u001b[0m       ])\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    325\u001b[0m       ])\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__make_cmp_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "mc = ModelCheckpoint('../models/49_1split.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, x_train, validation_data=(x_test, x_test), epochs=100, batch_size=32, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "def embed_and_translate(data, n_width, n_height):\n",
    "    ndata = np.zeros((len(data), n_width, n_height, 1), dtype='float32')\n",
    "    translations = np.empty((len(data), 2), dtype='float32')\n",
    "    width, height = data.shape[1], data.shape[2]\n",
    "    for i in range(len(data)):\n",
    "        x = np.random.randint(n_width-width)\n",
    "        y = np.random.randint(n_height-height)\n",
    "        ndata[i][x:x+width, y:y+height] = data[i] # rows, cols = height, width\n",
    "        translations[i][0] = x+(width//2)\n",
    "        translations[i][1] = y+(height//2)\n",
    "    return ndata, translations\n",
    "            \n",
    "n_splits = 16\n",
    "output_shape = (56, 56, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "latent_dim = 4\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_augmented, y_train_regr = embed_and_translate(x_train, output_shape[0], output_shape[1])\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train_augmented], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_augmented, y_test_regr = embed_and_translate(x_test, output_shape[0], output_shape[1])\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test_augmented], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "        self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "        self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        y_pred['decoder_out'] = tf.image.extract_glimpse(y_pred['decoder_out'], (28,28), y['regressor_out'], centered=False, normalized=False, noise='zero')\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "        y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "        y_regr = self.regress(z ,training=training)\n",
    "        return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=1, kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Activation('sigmoid')\n",
    "#             Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    'decoder_out' : 'mse',\n",
    "    'classifier_out' : 'categorical_crossentropy',\n",
    "    'regressor_out' : 'mean_absolute_percentage_error'\n",
    "}\n",
    "loss_weights = {\n",
    "    'decoder_out' : 0.0,\n",
    "    'classifier_out' : 0.0,\n",
    "    'regressor_out' : 1.0\n",
    "}\n",
    "model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "model.compile(loss=losses, loss_weights=loss_weights, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 9.1226 - classifier_out_loss: 2.3198 - decoder_out_loss: 0.2462 - regressor_out_loss: 9.1226 - val_loss: 3.5514 - val_classifier_out_loss: 2.3179 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 3.5514\n",
      "Epoch 2/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 3.4706 - classifier_out_loss: 2.3182 - decoder_out_loss: 0.2462 - regressor_out_loss: 3.4706 - val_loss: 2.9008 - val_classifier_out_loss: 2.3176 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 2.9008\n",
      "Epoch 3/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 2.9552 - classifier_out_loss: 2.3179 - decoder_out_loss: 0.2461 - regressor_out_loss: 2.9552 - val_loss: 2.7744 - val_classifier_out_loss: 2.3171 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 2.7744\n",
      "Epoch 4/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 2.5558 - classifier_out_loss: 2.3178 - decoder_out_loss: 0.2461 - regressor_out_loss: 2.5558 - val_loss: 2.7857 - val_classifier_out_loss: 2.3174 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 2.7857\n",
      "Epoch 5/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 2.2614 - classifier_out_loss: 2.3177 - decoder_out_loss: 0.2461 - regressor_out_loss: 2.2614 - val_loss: 2.1488 - val_classifier_out_loss: 2.3173 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 2.1488\n",
      "Epoch 6/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 2.0258 - classifier_out_loss: 2.3175 - decoder_out_loss: 0.2461 - regressor_out_loss: 2.0258 - val_loss: 2.0660 - val_classifier_out_loss: 2.3170 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 2.0660\n",
      "Epoch 7/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.9147 - classifier_out_loss: 2.3174 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.9147 - val_loss: 1.6536 - val_classifier_out_loss: 2.3169 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.6536\n",
      "Epoch 8/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.8348 - classifier_out_loss: 2.3173 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.8348 - val_loss: 1.6303 - val_classifier_out_loss: 2.3169 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.6303\n",
      "Epoch 9/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.8050 - classifier_out_loss: 2.3172 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.8050 - val_loss: 1.6272 - val_classifier_out_loss: 2.3169 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.6272\n",
      "Epoch 10/25\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 1.7171 - classifier_out_loss: 2.3171 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.7171 - val_loss: 2.6199 - val_classifier_out_loss: 2.3167 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 2.6199\n",
      "Epoch 11/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.6747 - classifier_out_loss: 2.3171 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.6747 - val_loss: 1.7424 - val_classifier_out_loss: 2.3168 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.7424\n",
      "Epoch 12/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.6669 - classifier_out_loss: 2.3170 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.6669 - val_loss: 1.5145 - val_classifier_out_loss: 2.3164 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5145\n",
      "Epoch 13/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.6150 - classifier_out_loss: 2.3170 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.6150 - val_loss: 1.8771 - val_classifier_out_loss: 2.3168 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.8771\n",
      "Epoch 14/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.6108 - classifier_out_loss: 2.3169 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.6108 - val_loss: 1.3841 - val_classifier_out_loss: 2.3167 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.3841\n",
      "Epoch 15/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.6005 - classifier_out_loss: 2.3169 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.6005 - val_loss: 1.3800 - val_classifier_out_loss: 2.3162 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.3800\n",
      "Epoch 16/25\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 1.5860 - classifier_out_loss: 2.3169 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5860 - val_loss: 1.4780 - val_classifier_out_loss: 2.3168 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.4780\n",
      "Epoch 17/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.5646 - classifier_out_loss: 2.3169 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5646 - val_loss: 1.5906 - val_classifier_out_loss: 2.3165 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5906\n",
      "Epoch 18/25\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 1.5499 - classifier_out_loss: 2.3168 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5499 - val_loss: 1.5408 - val_classifier_out_loss: 2.3165 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5408\n",
      "Epoch 19/25\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 1.5438 - classifier_out_loss: 2.3168 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5438 - val_loss: 1.5889 - val_classifier_out_loss: 2.3164 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5889\n",
      "Epoch 20/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 1.5275 - classifier_out_loss: 2.3168 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5275 - val_loss: 1.3238 - val_classifier_out_loss: 2.3166 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.3238\n",
      "Epoch 21/25\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 1.5243 - classifier_out_loss: 2.3168 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5243 - val_loss: 1.5644 - val_classifier_out_loss: 2.3163 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5644\n",
      "Epoch 22/25\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 1.5227 - classifier_out_loss: 2.3168 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.5227 - val_loss: 1.4897 - val_classifier_out_loss: 2.3164 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.4897\n",
      "Epoch 23/25\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 1.4821 - classifier_out_loss: 2.3167 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.4821 - val_loss: 1.5916 - val_classifier_out_loss: 2.3167 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5916\n",
      "Epoch 24/25\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 1.4989 - classifier_out_loss: 2.3168 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.4989 - val_loss: 1.5224 - val_classifier_out_loss: 2.3166 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.5224\n",
      "Epoch 25/25\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 1.4731 - classifier_out_loss: 2.3167 - decoder_out_loss: 0.2461 - regressor_out_loss: 1.4731 - val_loss: 1.3431 - val_classifier_out_loss: 2.3164 - val_decoder_out_loss: 0.2321 - val_regressor_out_loss: 1.3431\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n",
    "                    validation_data=(x_test_split, {'decoder_out': x_test, 'classifier_out': y_test, 'regressor_out':y_test_regr}),\n",
    "                    epochs=25, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
