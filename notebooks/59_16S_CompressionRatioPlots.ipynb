{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation, Dropout, MaxPooling2D\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "class MultiSplit(Model):\n",
    "    def __init__(self, n_splits, latent_dim, input_shape, output_shape):\n",
    "        super(MultiSplit, self).__init__()\n",
    "        self.encoder = self._create_encoder(latent_dim, input_shape)\n",
    "        self.input_reshaper = utils.Reshaper((n_splits, *input_shape), input_shape)\n",
    "        self.latent_reshaper = utils.Reshaper([latent_dim], [n_splits * latent_dim])\n",
    "        self.decoder = self._create_decoder(n_splits * latent_dim, output_shape)\n",
    "        self.classifier = self._create_classifier(n_splits * latent_dim)\n",
    "        self.regressor = self._create_regressor(n_splits * latent_dim)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z = self.encode(x, training=training)\n",
    "        y_reco = self.decode(z ,training=training)\n",
    "        y_class = self.classify(z ,training=training)\n",
    "        y_regr = self.regress(z ,training=training)\n",
    "        return {\"decoder_out\": y_reco, \"classifier_out\": y_class, \"regressor_out\": y_regr}\n",
    "        \n",
    "    def encode(self, x, training=True):\n",
    "        return self.encoder( self.input_reshaper(x) , training)\n",
    "    \n",
    "    def decode(self, z, training=True):\n",
    "        return self.decoder( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def classify(self, z, training=True):\n",
    "        return self.classifier( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def regress(self, z, training=True):\n",
    "        return self.regressor( self.latent_reshaper(z) , training)\n",
    "    \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(units=latent_dim, activation='sigmoid')\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_decoder(self, latent_dim, io_shape, n_filters=[32,64]):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(io_shape[0]//4 * io_shape[1]//4 * 8),  #! Reduce amount of neurons by 4.\n",
    "            Reshape((io_shape[0]//4, io_shape[1]//4, 8)),\n",
    "            Conv2DTranspose(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2DTranspose(filters=1, kernel_size=(3,3), padding='same'),\n",
    "            Activation('sigmoid', name='decoder_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='softmax', name='classifier_out')\n",
    "        ])\n",
    "    \n",
    "    def _create_regressor(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(10, activation='relu'),\n",
    "            Dense(2, activation='linear', name='regressor_out')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### MODEL with latent dims 1 #########\n",
      "Best Acc:  0.11349999904632568 , Best MAPE:  27.39385986328125\n",
      "####### MODEL with latent dims 2 #########\n",
      "Best Acc:  0.11349999904632568 , Best MAPE:  27.350797653198242\n",
      "####### MODEL with latent dims 4 #########\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 16\n",
    "output_shape = (56, 56, 1)\n",
    "input_shape  = (14, 14, 1)\n",
    "split_x, split_y = 14, 14\n",
    "\n",
    "def embed_and_translate(data, n_width, n_height):\n",
    "    ndata = np.zeros((len(data), n_width, n_height, 1), dtype='float32')\n",
    "    translations = np.empty((len(data), 2), dtype='float32')\n",
    "    width, height = data.shape[1], data.shape[2]\n",
    "    for i in range(len(data)):\n",
    "        x = np.random.randint(n_width-width)\n",
    "        y = np.random.randint(n_height-height)\n",
    "        ndata[i][x:x+width, y:y+height] = data[i] # rows, cols = height, width\n",
    "        translations[i][0] = x+(width//2)\n",
    "        translations[i][1] = y+(height//2)\n",
    "    return ndata, translations\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train_augmented, y_train_regr = embed_and_translate(x_train, output_shape[0], output_shape[1])\n",
    "x_train_split = np.array([utils.split(x, split_x, split_y) for x in x_train_augmented], dtype='float32')\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test_augmented, y_test_regr = embed_and_translate(x_test, output_shape[0], output_shape[1])\n",
    "x_test_split = np.array([utils.split(x, split_x, split_y) for x in x_test_augmented], dtype='float32')\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "            \n",
    "for latent_dim in [1,2,4,6,12,24,49,98,196]:\n",
    "    \n",
    "    print(\"####### MODEL with latent dims %d #########\" % latent_dim)\n",
    "\n",
    "    csv_logger = callbacks.CSVLogger('../models/59/16x%d_train.log' % latent_dim)\n",
    "    model_checkpoint = callbacks.ModelCheckpoint('../models/59/16x%d_model_weights.h5' % latent_dim, monitor='val_classifier_out_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "    losses = {\n",
    "        'decoder_out' : 'mse',\n",
    "        'classifier_out' : 'categorical_crossentropy',\n",
    "        'regressor_out' : 'mean_absolute_percentage_error'\n",
    "    }\n",
    "    loss_weights = {\n",
    "        'decoder_out' : 0.0,\n",
    "        'classifier_out' : 1.0,\n",
    "        'regressor_out' : 1.0\n",
    "    }\n",
    "    model = MultiSplit(n_splits, latent_dim, input_shape, output_shape)\n",
    "    model.compile(loss=losses, loss_weights=loss_weights, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train_split, {'decoder_out': x_train_augmented, 'classifier_out': y_train, 'regressor_out': y_train_regr},\n",
    "              validation_data=(x_test_split, {'decoder_out': x_test_augmented, 'classifier_out': y_test, 'regressor_out':y_test_regr}),\n",
    "              epochs=1, batch_size=32, callbacks=[csv_logger, model_checkpoint], verbose=0)\n",
    "    best_idx = np.argmax(history.history['val_classifier_out_accuracy'])\n",
    "    print('Best Acc: ', list(history.history['val_classifier_out_accuracy'])[best_idx],', Best MAPE: ', list(history.history['val_regressor_out_loss'])[best_idx]) \n",
    "    \n",
    "    model = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
