{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv2D, Conv2DTranspose, Dense, Input, Reshape, concatenate, Activation\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys; sys.path.insert(0, '..')\n",
    "import utils\n",
    "\n",
    "\n",
    "class FourSplit(Model):\n",
    "    def __init__(self, n_enc, latent_dim, io_shape):\n",
    "        super(FourSplit, self).__init__()\n",
    "        n_filters = [32, 64]\n",
    "        self.encoder = self._create_encoder(latent_dim, io_shape, n_filters)\n",
    "        self.classifier = self._create_classifier(latent_dim)\n",
    "#         self.compile()\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder( x )\n",
    "    \n",
    "    def classify(self, z):\n",
    "        return self.classifier( z )\n",
    "        \n",
    "    def _create_encoder(self, latent_dim, input_shape, n_filters):\n",
    "        return Sequential([\n",
    "            Input(shape=input_shape),\n",
    "            Conv2D(filters=n_filters[0], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Conv2D(filters=n_filters[1], kernel_size=(3,3), strides=(2,2), activation='relu', padding='same'),\n",
    "            Flatten(),\n",
    "            Dense(latent_dim)\n",
    "        ], name='encoder')\n",
    "\n",
    "    def _create_classifier(self, latent_dim):\n",
    "        return Sequential([\n",
    "            Input(shape=(latent_dim)),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                50192     \n",
      "=================================================================\n",
      "Total params: 69,008\n",
      "Trainable params: 0\n",
      "Non-trainable params: 69,008\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 170\n",
      "Trainable params: 170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LATENT_DIM = 16\n",
    "N_ENC = 1\n",
    "IO_SHAPE = (28,28,1)\n",
    "\n",
    "train_data, test_data = utils.gen_data(N_ENC, BATCH_SIZE)\n",
    "model = FourSplit(N_ENC, LATENT_DIM, IO_SHAPE)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model.encoder.load_weights('../weights/CE16_epoch25_weights.h5')\n",
    "for layer in model.encoder.layers:\n",
    "    layer.trainable = False\n",
    "print(model.encoder.summary())\n",
    "print(model.classifier.summary())\n",
    "# print(model.trainable_variables[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 || train loss: 1.41782 | train acc: 0.58 time: 3.0s\n",
      "Epoch  2 || train loss: 0.54234 | train acc: 0.74 time: 1.7s\n",
      "Epoch  3 || train loss: 0.37918 | train acc: 0.80 time: 1.6s\n",
      "Epoch  4 || train loss: 0.31298 | train acc: 0.83 time: 1.6s\n",
      "Epoch  5 || train loss: 0.27868 | train acc: 0.85 time: 1.7s\n",
      "Epoch  6 || train loss: 0.25879 | train acc: 0.86 time: 1.6s\n",
      "Epoch  7 || train loss: 0.24671 | train acc: 0.87 time: 1.8s\n",
      "Epoch  8 || train loss: 0.23927 | train acc: 0.88 time: 1.7s\n",
      "Epoch  9 || train loss: 0.23398 | train acc: 0.89 time: 1.7s\n",
      "Epoch 10 || train loss: 0.23061 | train acc: 0.89 time: 1.7s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.metrics import Mean\n",
    "import numpy as np\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "# from keras.losses import categorical_crossentropy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "loss_fn = CategoricalCrossentropy(from_logits=False)\n",
    "acc = CategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y, optimizer):\n",
    "    with tf.GradientTape() as tape:   # tf.Variables inside are tracked\n",
    "        y_pred = model.classify( model.encode(x) )\n",
    "#         loss = categorical_crossentropy(y, y_pred, from_logits=False)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    acc.update_state(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "# for epoch in range(1, 31):\n",
    "for epoch in range(1, 11):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = Mean()\n",
    "    for i, (x, y) in enumerate(train_data):\n",
    "        loss = train_step(model, x, y, optimizer)\n",
    "        train_loss(loss)\n",
    "    train_losses.append(train_loss.result())\n",
    "    train_accs.append(acc.result())\n",
    "     \n",
    "    # Test\n",
    "#     test_loss = Mean()\n",
    "#     for i, (x, y) in enumerate(test_data):\n",
    "#         test_loss(model.class_loss(x, y))\n",
    "#     test_losses.append(test_loss.result())\n",
    "    \n",
    "    end_time = time.time()\n",
    "    delta = round(end_time - start_time, 1)\n",
    "#     print(\"Epoch {:2d} || train loss: {:.5f} | test loss: {:.5f} | time: {}s\".format(epoch, train_losses[epoch-1], test_losses[epoch-1], delta))\n",
    "    print(\"Epoch {:2d} || train loss: {:.5f} | train acc: {:.2f} | time: {}s\".format(epoch, train_losses[epoch-1], train_accs[epoch-1], delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
